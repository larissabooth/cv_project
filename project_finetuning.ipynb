{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larissabooth/cv_project/blob/main/project_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89hdk3d7N-hb"
      },
      "source": [
        "##### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hZKgiJlylspb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "476a0649-3188-4343-c376-0ab61828037a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9oIl-rCOypT"
      },
      "source": [
        "## SimCLR: A Simple Framework for Contrastive Learning of Visual Representations\n",
        "\n",
        "This colab demonstrates how to load pretrained/finetuned SimCLR models from hub modules for fine-tuning\n",
        "\n",
        "The checkpoints are accessible in the following Google Cloud Storage folders.\n",
        "\n",
        "* Pretrained SimCLRv2 models with a linear classifier: [gs://simclr-checkpoints/simclrv2/pretrained](https://console.cloud.google.com/storage/browser/simclr-checkpoints/simclrv2/pretrained)\n",
        "* Fine-tuned SimCLRv2 models on 1% of labels: [gs://simclr-checkpoints/simclrv2/finetuned_1pct](https://console.cloud.google.com/storage/browser/simclr-checkpoints/simclrv2/finetuned_1pct)\n",
        "* Fine-tuned SimCLRv2 models on 10% of labels: [gs://simclr-checkpoints/simclrv2/finetuned_10pct](https://console.cloud.google.com/storage/browser/simclr-checkpoints/simclrv2/finetuned_10pct)\n",
        "* Fine-tuned SimCLRv2 models on 100% of labels: [gs://simclr-checkpoints/simclrv2/finetuned_100pct](https://console.cloud.google.com/storage/browser/simclr-checkpoints/simclrv2/finetuned_100pct)\n",
        "* Supervised models with the same architectures: [gs://simclr-checkpoints/simclrv2/pretrained](https://console.cloud.google.com/storage/browser/simclr-checkpoints/simclrv2/pretrained)\n",
        "\n",
        "Use the corresponding checkpoint / hub-module paths for accessing the model. For example, to use a pre-trained model (with a linear classifier) with ResNet-152 (2x+SK), set the path to `gs://simclr-checkpoints/simclrv2/pretrained/r152_2x_sk1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih5NlvdDEOI1"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Auto Arborist Code**"
      ],
      "metadata": {
        "id": "Z8Os6aqZmHoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This defines the structure of the features in the TF Examples\n",
        "\n",
        "features = {\n",
        "      'tree/city':\n",
        "          tf.io.FixedLenFeature((), tf.string),\n",
        "      'tree/id':\n",
        "          tf.io.FixedLenFeature((), tf.string),\n",
        "      'tree/idx':\n",
        "          tf.io.FixedLenFeature((), tf.string),\n",
        "      'tree/genus/label':\n",
        "          tf.io.FixedLenFeature((), tf.int64),\n",
        "      'tree/genus/genus':\n",
        "          tf.io.FixedLenFeature((), tf.string),\n",
        "      'tree/latitude':\n",
        "          tf.io.FixedLenFeature((), tf.float32),\n",
        "      'tree/longitude':\n",
        "          tf.io.FixedLenFeature((), tf.float32),\n",
        "      'aerial/encoded': tf.io.FixedLenFeature((), tf.string),\n",
        "      'streetlevel/encoded': tf.io.FixedLenFeature((), tf.string),\n",
        "  }"
      ],
      "metadata": {
        "id": "TEsVpu1dmQZU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnyvq6g-P2rW",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd8cc67-838e-444f-f32c-44edbf30a0b8"
      },
      "source": [
        "#@title Load class id to label text mapping from big_transfer (hidden)\n",
        "# Code snippet credit: https://github.com/google-research/big_transfer\n",
        "\n",
        "!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\n",
        "\n",
        "imagenet_int_to_str = {}\n",
        "\n",
        "with open('ilsvrc2012_wordnet_lemmas.txt', 'r') as f:\n",
        "  for i in range(1000):\n",
        "    row = f.readline()\n",
        "    row = row.rstrip()\n",
        "    imagenet_int_to_str.update({i: row})\n",
        "\n",
        "tf_flowers_labels = ['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-05 15:57:22--  https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.212.128, 172.253.114.128, 108.177.111.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.212.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21675 (21K) [text/plain]\n",
            "Saving to: ‘ilsvrc2012_wordnet_lemmas.txt.8’\n",
            "\n",
            "ilsvrc2012_wordnet_ 100%[===================>]  21.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-05 15:57:23 (62.9 MB/s) - ‘ilsvrc2012_wordnet_lemmas.txt.8’ saved [21675/21675]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxhfMmVdHoZM",
        "cellView": "form"
      },
      "source": [
        "#@title Preprocessing functions from data_util.py in SimCLR repository (hidden).\n",
        "\n",
        "FLAGS_color_jitter_strength = 0.3\n",
        "CROP_PROPORTION = 0.875  # Standard for ImageNet.\n",
        "\n",
        "\n",
        "def random_apply(func, p, x):\n",
        "  \"\"\"Randomly apply function func to x with probability p.\"\"\"\n",
        "  return tf.cond(\n",
        "      tf.less(tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
        "              tf.cast(p, tf.float32)),\n",
        "      lambda: func(x),\n",
        "      lambda: x)\n",
        "\n",
        "\n",
        "def random_brightness(image, max_delta, impl='simclrv2'):\n",
        "  \"\"\"A multiplicative vs additive change of brightness.\"\"\"\n",
        "  if impl == 'simclrv2':\n",
        "    factor = tf.random_uniform(\n",
        "        [], tf.maximum(1.0 - max_delta, 0), 1.0 + max_delta)\n",
        "    image = image * factor\n",
        "  elif impl == 'simclrv1':\n",
        "    image = random_brightness(image, max_delta=max_delta)\n",
        "  else:\n",
        "    raise ValueError('Unknown impl {} for random brightness.'.format(impl))\n",
        "  return image\n",
        "\n",
        "\n",
        "def to_grayscale(image, keep_channels=True):\n",
        "  image = tf.image.rgb_to_grayscale(image)\n",
        "  if keep_channels:\n",
        "    image = tf.tile(image, [1, 1, 3])\n",
        "  return image\n",
        "\n",
        "\n",
        "def color_jitter(image,\n",
        "                 strength,\n",
        "                 random_order=True):\n",
        "  \"\"\"Distorts the color of the image.\n",
        "  Args:\n",
        "    image: The input image tensor.\n",
        "    strength: the floating number for the strength of the color augmentation.\n",
        "    random_order: A bool, specifying whether to randomize the jittering order.\n",
        "  Returns:\n",
        "    The distorted image tensor.\n",
        "  \"\"\"\n",
        "  brightness = 0.8 * strength\n",
        "  contrast = 0.8 * strength\n",
        "  saturation = 0.8 * strength\n",
        "  hue = 0.2 * strength\n",
        "  if random_order:\n",
        "    return color_jitter_rand(image, brightness, contrast, saturation, hue)\n",
        "  else:\n",
        "    return color_jitter_nonrand(image, brightness, contrast, saturation, hue)\n",
        "\n",
        "\n",
        "def color_jitter_nonrand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "  \"\"\"Distorts the color of the image (jittering order is fixed).\n",
        "  Args:\n",
        "    image: The input image tensor.\n",
        "    brightness: A float, specifying the brightness for color jitter.\n",
        "    contrast: A float, specifying the contrast for color jitter.\n",
        "    saturation: A float, specifying the saturation for color jitter.\n",
        "    hue: A float, specifying the hue for color jitter.\n",
        "  Returns:\n",
        "    The distorted image tensor.\n",
        "  \"\"\"\n",
        "  with tf.name_scope('distort_color'):\n",
        "    def apply_transform(i, x, brightness, contrast, saturation, hue):\n",
        "      \"\"\"Apply the i-th transformation.\"\"\"\n",
        "      if brightness != 0 and i == 0:\n",
        "        x = random_brightness(x, max_delta=brightness)\n",
        "      elif contrast != 0 and i == 1:\n",
        "        x = tf.image.random_contrast(\n",
        "            x, lower=1-contrast, upper=1+contrast)\n",
        "      elif saturation != 0 and i == 2:\n",
        "        x = tf.image.random_saturation(\n",
        "            x, lower=1-saturation, upper=1+saturation)\n",
        "      elif hue != 0:\n",
        "        x = tf.image.random_hue(x, max_delta=hue)\n",
        "      return x\n",
        "\n",
        "    for i in range(4):\n",
        "      image = apply_transform(i, image, brightness, contrast, saturation, hue)\n",
        "      image = tf.clip_by_value(image, 0., 1.)\n",
        "    return image\n",
        "\n",
        "\n",
        "def color_jitter_rand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "  \"\"\"Distorts the color of the image (jittering order is random).\n",
        "  Args:\n",
        "    image: The input image tensor.\n",
        "    brightness: A float, specifying the brightness for color jitter.\n",
        "    contrast: A float, specifying the contrast for color jitter.\n",
        "    saturation: A float, specifying the saturation for color jitter.\n",
        "    hue: A float, specifying the hue for color jitter.\n",
        "  Returns:\n",
        "    The distorted image tensor.\n",
        "  \"\"\"\n",
        "  with tf.name_scope('distort_color'):\n",
        "    def apply_transform(i, x):\n",
        "      \"\"\"Apply the i-th transformation.\"\"\"\n",
        "      def brightness_foo():\n",
        "        if brightness == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return random_brightness(x, max_delta=brightness)\n",
        "      def contrast_foo():\n",
        "        if contrast == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_contrast(x, lower=1-contrast, upper=1+contrast)\n",
        "      def saturation_foo():\n",
        "        if saturation == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_saturation(\n",
        "              x, lower=1-saturation, upper=1+saturation)\n",
        "      def hue_foo():\n",
        "        if hue == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_hue(x, max_delta=hue)\n",
        "      x = tf.cond(tf.less(i, 2),\n",
        "                  lambda: tf.cond(tf.less(i, 1), brightness_foo, contrast_foo),\n",
        "                  lambda: tf.cond(tf.less(i, 3), saturation_foo, hue_foo))\n",
        "      return x\n",
        "\n",
        "    perm = tf.random_shuffle(tf.range(4))\n",
        "    for i in range(4):\n",
        "      image = apply_transform(perm[i], image)\n",
        "      image = tf.clip_by_value(image, 0., 1.)\n",
        "    return image\n",
        "\n",
        "\n",
        "def _compute_crop_shape(\n",
        "    image_height, image_width, aspect_ratio, crop_proportion):\n",
        "  \"\"\"Compute aspect ratio-preserving shape for central crop.\n",
        "  The resulting shape retains `crop_proportion` along one side and a proportion\n",
        "  less than or equal to `crop_proportion` along the other side.\n",
        "  Args:\n",
        "    image_height: Height of image to be cropped.\n",
        "    image_width: Width of image to be cropped.\n",
        "    aspect_ratio: Desired aspect ratio (width / height) of output.\n",
        "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
        "  Returns:\n",
        "    crop_height: Height of image after cropping.\n",
        "    crop_width: Width of image after cropping.\n",
        "  \"\"\"\n",
        "  image_width_float = tf.cast(image_width, tf.float32)\n",
        "  image_height_float = tf.cast(image_height, tf.float32)\n",
        "\n",
        "  def _requested_aspect_ratio_wider_than_image():\n",
        "    crop_height = tf.cast(tf.rint(\n",
        "        crop_proportion / aspect_ratio * image_width_float), tf.int32)\n",
        "    crop_width = tf.cast(tf.rint(\n",
        "        crop_proportion * image_width_float), tf.int32)\n",
        "    return crop_height, crop_width\n",
        "\n",
        "  def _image_wider_than_requested_aspect_ratio():\n",
        "    crop_height = tf.cast(\n",
        "        tf.rint(crop_proportion * image_height_float), tf.int32)\n",
        "    crop_width = tf.cast(tf.rint(\n",
        "        crop_proportion * aspect_ratio *\n",
        "        image_height_float), tf.int32)\n",
        "    return crop_height, crop_width\n",
        "\n",
        "  return tf.cond(\n",
        "      aspect_ratio > image_width_float / image_height_float,\n",
        "      _requested_aspect_ratio_wider_than_image,\n",
        "      _image_wider_than_requested_aspect_ratio)\n",
        "\n",
        "\n",
        "def center_crop(image, height, width, crop_proportion):\n",
        "  \"\"\"Crops to center of image and rescales to desired size.\n",
        "  Args:\n",
        "    image: Image Tensor to crop.\n",
        "    height: Height of image to be cropped.\n",
        "    width: Width of image to be cropped.\n",
        "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
        "  Returns:\n",
        "    A `height` x `width` x channels Tensor holding a central crop of `image`.\n",
        "  \"\"\"\n",
        "  shape = tf.shape(image)\n",
        "  image_height = shape[0]\n",
        "  image_width = shape[1]\n",
        "  crop_height, crop_width = _compute_crop_shape(\n",
        "      image_height, image_width, height / width, crop_proportion)\n",
        "  offset_height = ((image_height - crop_height) + 1) // 2\n",
        "  offset_width = ((image_width - crop_width) + 1) // 2\n",
        "  image = tf.image.crop_to_bounding_box(\n",
        "      image, offset_height, offset_width, crop_height, crop_width)\n",
        "\n",
        "  image = tf.image.resize_bicubic([image], [height, width])[0]\n",
        "\n",
        "  return image\n",
        "\n",
        "\n",
        "def distorted_bounding_box_crop(image,\n",
        "                                bbox,\n",
        "                                min_object_covered=0.1,\n",
        "                                aspect_ratio_range=(0.75, 1.33),\n",
        "                                area_range=(0.05, 1.0),\n",
        "                                max_attempts=100,\n",
        "                                scope=None):\n",
        "  \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n",
        "  See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
        "  Args:\n",
        "    image: `Tensor` of image data.\n",
        "    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n",
        "        where each coordinate is [0, 1) and the coordinates are arranged\n",
        "        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n",
        "        image.\n",
        "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
        "        area of the image must contain at least this fraction of any bounding\n",
        "        box supplied.\n",
        "    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n",
        "        image must have an aspect ratio = width / height within this range.\n",
        "    area_range: An optional list of `float`s. The cropped area of the image\n",
        "        must contain a fraction of the supplied image within in this range.\n",
        "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
        "        region of the image of the specified constraints. After `max_attempts`\n",
        "        failures, return the entire image.\n",
        "    scope: Optional `str` for name scope.\n",
        "  Returns:\n",
        "    (cropped image `Tensor`, distorted bbox `Tensor`).\n",
        "  \"\"\"\n",
        "  with tf.name_scope(scope, 'distorted_bounding_box_crop', [image, bbox]):\n",
        "    shape = tf.shape(image)\n",
        "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
        "        shape,\n",
        "        bounding_boxes=bbox,\n",
        "        min_object_covered=min_object_covered,\n",
        "        aspect_ratio_range=aspect_ratio_range,\n",
        "        area_range=area_range,\n",
        "        max_attempts=max_attempts,\n",
        "        use_image_if_no_bounding_boxes=True)\n",
        "    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
        "\n",
        "    # Crop the image to the specified bounding box.\n",
        "    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
        "    target_height, target_width, _ = tf.unstack(bbox_size)\n",
        "    image = tf.image.crop_to_bounding_box(\n",
        "        image, offset_y, offset_x, target_height, target_width)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def crop_and_resize(image, height, width):\n",
        "  \"\"\"Make a random crop and resize it to height `height` and width `width`.\n",
        "  Args:\n",
        "    image: Tensor representing the image.\n",
        "    height: Desired image height.\n",
        "    width: Desired image width.\n",
        "  Returns:\n",
        "    A `height` x `width` x channels Tensor holding a random crop of `image`.\n",
        "  \"\"\"\n",
        "  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n",
        "  aspect_ratio = width / height\n",
        "  image = distorted_bounding_box_crop(\n",
        "      image,\n",
        "      bbox,\n",
        "      min_object_covered=0.1,\n",
        "      aspect_ratio_range=(3. / 4 * aspect_ratio, 4. / 3. * aspect_ratio),\n",
        "      area_range=(0.08, 1.0),\n",
        "      max_attempts=100,\n",
        "      scope=None)\n",
        "  return tf.image.resize_bicubic([image], [height, width])[0]\n",
        "\n",
        "\n",
        "def gaussian_blur(image, kernel_size, sigma, padding='SAME'):\n",
        "  \"\"\"Blurs the given image with separable convolution.\n",
        "  Args:\n",
        "    image: Tensor of shape [height, width, channels] and dtype float to blur.\n",
        "    kernel_size: Integer Tensor for the size of the blur kernel. This is should\n",
        "      be an odd number. If it is an even number, the actual kernel size will be\n",
        "      size + 1.\n",
        "    sigma: Sigma value for gaussian operator.\n",
        "    padding: Padding to use for the convolution. Typically 'SAME' or 'VALID'.\n",
        "  Returns:\n",
        "    A Tensor representing the blurred image.\n",
        "  \"\"\"\n",
        "  radius = tf.to_int32(kernel_size / 2)\n",
        "  kernel_size = radius * 2 + 1\n",
        "  x = tf.to_float(tf.range(-radius, radius + 1))\n",
        "  blur_filter = tf.exp(\n",
        "      -tf.pow(x, 2.0) / (2.0 * tf.pow(tf.to_float(sigma), 2.0)))\n",
        "  blur_filter /= tf.reduce_sum(blur_filter)\n",
        "  # One vertical and one horizontal filter.\n",
        "  blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n",
        "  blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n",
        "  num_channels = tf.shape(image)[-1]\n",
        "  blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
        "  blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
        "  expand_batch_dim = image.shape.ndims == 3\n",
        "  if expand_batch_dim:\n",
        "    # Tensorflow requires batched input to convolutions, which we can fake with\n",
        "    # an extra dimension.\n",
        "    image = tf.expand_dims(image, axis=0)\n",
        "  blurred = tf.nn.depthwise_conv2d(\n",
        "      image, blur_h, strides=[1, 1, 1, 1], padding=padding)\n",
        "  blurred = tf.nn.depthwise_conv2d(\n",
        "      blurred, blur_v, strides=[1, 1, 1, 1], padding=padding)\n",
        "  if expand_batch_dim:\n",
        "    blurred = tf.squeeze(blurred, axis=0)\n",
        "  return blurred\n",
        "\n",
        "\n",
        "def random_crop_with_resize(image, height, width, p=1.0):\n",
        "  \"\"\"Randomly crop and resize an image.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    p: Probability of applying this transformation.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  def _transform(image):  # pylint: disable=missing-docstring\n",
        "    image = crop_and_resize(image, height, width)\n",
        "    return image\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def random_color_jitter(image, p=1.0):\n",
        "  def _transform(image):\n",
        "    color_jitter_t = functools.partial(\n",
        "        color_jitter, strength=FLAGS_color_jitter_strength)\n",
        "    image = random_apply(color_jitter_t, p=0.8, x=image)\n",
        "    return random_apply(to_grayscale, p=0.2, x=image)\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def random_blur(image, height, width, p=1.0):\n",
        "  \"\"\"Randomly blur an image.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    p: probability of applying this transformation.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  del width\n",
        "  def _transform(image):\n",
        "    sigma = tf.random.uniform([], 0.1, 2.0, dtype=tf.float32)\n",
        "    return gaussian_blur(\n",
        "        image, kernel_size=height//10, sigma=sigma, padding='SAME')\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def batch_random_blur(images_list, height, width, blur_probability=0.5):\n",
        "  \"\"\"Apply efficient batch data transformations.\n",
        "  Args:\n",
        "    images_list: a list of image tensors.\n",
        "    height: the height of image.\n",
        "    width: the width of image.\n",
        "    blur_probability: the probaility to apply the blur operator.\n",
        "  Returns:\n",
        "    Preprocessed feature list.\n",
        "  \"\"\"\n",
        "  def generate_selector(p, bsz):\n",
        "    shape = [bsz, 1, 1, 1]\n",
        "    selector = tf.cast(\n",
        "        tf.less(tf.random_uniform(shape, 0, 1, dtype=tf.float32), p),\n",
        "        tf.float32)\n",
        "    return selector\n",
        "\n",
        "  new_images_list = []\n",
        "  for images in images_list:\n",
        "    images_new = random_blur(images, height, width, p=1.)\n",
        "    selector = generate_selector(blur_probability, tf.shape(images)[0])\n",
        "    images = images_new * selector + images * (1 - selector)\n",
        "    images = tf.clip_by_value(images, 0., 1.)\n",
        "    new_images_list.append(images)\n",
        "\n",
        "  return new_images_list\n",
        "\n",
        "def random_crop(image, height, width, crop_proportion, p=1.0):\n",
        "  \"\"\"Randomly crops the image while ensuring at least 80% of the original image is retained.\n",
        "\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
        "    p: Probability of applying this transformation.\n",
        "\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "\n",
        "  def _transform(image):\n",
        "    shape = tf.shape(image)\n",
        "    image_height = shape[0]\n",
        "    image_width = shape[1]\n",
        "    crop_height, crop_width = _compute_crop_shape(image_height, image_width, height / width, crop_proportion)\n",
        "    offset_height = tf.random.uniform([], 0, image_height - crop_height + 1, dtype=tf.int32)\n",
        "    offset_width = tf.random.uniform([], 0, image_width - crop_width + 1, dtype=tf.int32)\n",
        "    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, crop_height, crop_width)\n",
        "    image = tf.image.resize_bicubic([image], [height, width])[0]\n",
        "    return image\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "def center_pixel_crop(image, center_pixel, height, width, crop_proportion):\n",
        "  \"\"\"Crops to center of image and rescales to desired size.\n",
        "  Args:\n",
        "    image: Image Tensor to crop.\n",
        "    center_pixel: Tuple (x,y) coordinates of center pixel.\n",
        "    height: Height of image to be cropped.\n",
        "    width: Width of image to be cropped.\n",
        "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
        "  Returns:\n",
        "    A `height` x `width` x channels Tensor holding a central crop of `image`.\n",
        "  \"\"\"\n",
        "  shape = tf.shape(image)\n",
        "  image_height = shape[0]\n",
        "  image_width = shape[1]\n",
        "  print(\"Image shape\")\n",
        "  print(image_height, image_width)\n",
        "  crop_height, crop_width = _compute_crop_shape(\n",
        "      image_height, image_width, height / width, crop_proportion)\n",
        "  \n",
        "  center_x = tf.cast(center_pixel[0], dtype = tf.int32)\n",
        "  center_y = tf.cast(center_pixel[1], dtype = tf.int32)\n",
        "  \n",
        "  offset_height = center_x - crop_height // 2\n",
        "  offset_width = center_y - crop_width // 2\n",
        "\n",
        "  if offset_height < 0:\n",
        "    offset_height = 0\n",
        "\n",
        "  if offset_width < 0:\n",
        "    offset_width = 0\n",
        "\n",
        "  if offset_height + crop_height > image_height:\n",
        "    offset_height = image_height - crop_height\n",
        "\n",
        "  if offset_width + crop_width > image_width:\n",
        "    offset_width = image_width - crop_width\n",
        "\n",
        "  print(\"Crop shape\")\n",
        "  print(crop_height, crop_width)\n",
        "\n",
        "  print(\"Offset shape\")\n",
        "  print(offset_height, offset_width)\n",
        "  image = tf.image.crop_to_bounding_box(\n",
        "      image, offset_height, offset_width, crop_height, crop_width)\n",
        "\n",
        "  image = tf.image.resize_bicubic([image], [height, width])[0]\n",
        "\n",
        "  return image\n",
        "\n",
        "\n",
        "# def center_crop(image, height, width, crop_proportion):\n",
        "#   \"\"\"Crops to center of image and rescales to desired size.\n",
        "#   Args:\n",
        "#     image: Image Tensor to crop.\n",
        "#     height: Height of image to be cropped.\n",
        "#     width: Width of image to be cropped.\n",
        "#     crop_proportion: Proportion of image to retain along the less-cropped side.\n",
        "#   Returns:\n",
        "#     A `height` x `width` x channels Tensor holding a central crop of `image`.\n",
        "#   \"\"\"\n",
        "#   shape = tf.shape(image)\n",
        "#   image_height = shape[0]\n",
        "#   image_width = shape[1]\n",
        "#   crop_height, crop_width = _compute_crop_shape(\n",
        "#       image_height, image_width, height / width, crop_proportion)\n",
        "#   offset_height = ((image_height - crop_height) + 1) // 2\n",
        "#   offset_width = ((image_width - crop_width) + 1) // 2\n",
        "#   image = tf.image.crop_to_bounding_box(\n",
        "#       image, offset_height, offset_width, crop_height, crop_width)\n",
        "\n",
        "#   image = tf.image.resize_bicubic([image], [height, width])[0]\n",
        "\n",
        "#   return image\n",
        "\n",
        "def preprocess_for_train(image, height, width, center_pixel,\n",
        "                         color_distort=True, crop=True, flip=True):\n",
        "  \"\"\"Preprocesses the given image for training.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    color_distort: Whether to apply the color distortion.\n",
        "    crop: Whether to crop the image.\n",
        "    flip: Whether or not to flip left and right of an image.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  if crop:\n",
        "    # image = random_crop_with_resize(image, height, width)\n",
        "    # image = random_crop(image, height, width, crop_proportion=0.8)\n",
        "    image = center_pixel_crop(image, center_pixel, height, width, crop_proportion=1.0)\n",
        "  if flip:\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "  if color_distort:\n",
        "    image = random_color_jitter(image)\n",
        "  image = tf.reshape(image, [height, width, 3])\n",
        "  image = tf.clip_by_value(image, 0., 1.)\n",
        "  return image\n",
        "\n",
        "\n",
        "def preprocess_for_eval(image, height, width, crop=True):\n",
        "  \"\"\"Preprocesses the given image for evaluation.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    crop: Whether or not to (center) crop the test images.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  if crop:\n",
        "    image = center_crop(image, height, width, crop_proportion=CROP_PROPORTION)\n",
        "  image = tf.reshape(image, [height, width, 3])\n",
        "  image = tf.clip_by_value(image, 0., 1.)\n",
        "  return image\n",
        "\n",
        "\n",
        "def preprocess_image(image, height, width,center_pixel, is_training=False,\n",
        "                     color_distort=True, test_crop=True):\n",
        "  \"\"\"Preprocesses the given image.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    is_training: `bool` for whether the preprocessing is for training.\n",
        "    color_distort: whether to apply the color distortion.\n",
        "    test_crop: whether or not to extract a central crop of the images\n",
        "        (as for standard ImageNet evaluation) during the evaluation.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor` of range [0, 1].\n",
        "  \"\"\"\n",
        "  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "  if is_training:\n",
        "    return preprocess_for_train(image, height, width,center_pixel, color_distort)\n",
        "  else:\n",
        "    return preprocess_for_eval(image, height, width, test_crop)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhXTUoUs_9zd",
        "cellView": "form"
      },
      "source": [
        "#@title LARS optimizer from data_util.py in SimCLR repository (hidden).\n",
        "\n",
        "EETA_DEFAULT = 0.001\n",
        "\n",
        "class LARSOptimizer(tf.train.Optimizer):\n",
        "  \"\"\"Layer-wise Adaptive Rate Scaling for large batch training.\n",
        "\n",
        "  Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n",
        "  I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               learning_rate,\n",
        "               momentum=0.9,\n",
        "               use_nesterov=False,\n",
        "               weight_decay=0.0,\n",
        "               exclude_from_weight_decay=None,\n",
        "               exclude_from_layer_adaptation=None,\n",
        "               classic_momentum=True,\n",
        "               eeta=EETA_DEFAULT,\n",
        "               name=\"LARSOptimizer\"):\n",
        "    \"\"\"Constructs a LARSOptimizer.\n",
        "\n",
        "    Args:\n",
        "      learning_rate: A `float` for learning rate.\n",
        "      momentum: A `float` for momentum.\n",
        "      use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n",
        "      weight_decay: A `float` for weight decay.\n",
        "      exclude_from_weight_decay: A list of `string` for variable screening, if\n",
        "          any of the string appears in a variable's name, the variable will be\n",
        "          excluded for computing weight decay. For example, one could specify\n",
        "          the list like ['batch_normalization', 'bias'] to exclude BN and bias\n",
        "          from weight decay.\n",
        "      exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n",
        "          for layer adaptation. If it is None, it will be defaulted the same as\n",
        "          exclude_from_weight_decay.\n",
        "      classic_momentum: A `boolean` for whether to use classic (or popular)\n",
        "          momentum. The learning rate is applied during momeuntum update in\n",
        "          classic momentum, but after momentum for popular momentum.\n",
        "      eeta: A `float` for scaling of learning rate when computing trust ratio.\n",
        "      name: The name for the scope.\n",
        "    \"\"\"\n",
        "    super(LARSOptimizer, self).__init__(False, name)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.momentum = momentum\n",
        "    self.weight_decay = weight_decay\n",
        "    self.use_nesterov = use_nesterov\n",
        "    self.classic_momentum = classic_momentum\n",
        "    self.eeta = eeta\n",
        "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "    # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n",
        "    # arg is None.\n",
        "    if exclude_from_layer_adaptation:\n",
        "      self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
        "    else:\n",
        "      self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
        "\n",
        "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
        "    if global_step is None:\n",
        "      global_step = tf.train.get_or_create_global_step()\n",
        "    new_global_step = global_step + 1\n",
        "\n",
        "    assignments = []\n",
        "    for (grad, param) in grads_and_vars:\n",
        "      if grad is None or param is None:\n",
        "        continue\n",
        "\n",
        "      param_name = param.op.name\n",
        "\n",
        "      v = tf.get_variable(\n",
        "          name=param_name + \"/Momentum\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "\n",
        "      if self._use_weight_decay(param_name):\n",
        "        grad += self.weight_decay * param\n",
        "\n",
        "      if self.classic_momentum:\n",
        "        trust_ratio = 1.0\n",
        "        if self._do_layer_adaptation(param_name):\n",
        "          w_norm = tf.norm(param, ord=2)\n",
        "          g_norm = tf.norm(grad, ord=2)\n",
        "          trust_ratio = tf.where(\n",
        "              tf.greater(w_norm, 0), tf.where(\n",
        "                  tf.greater(g_norm, 0), (self.eeta * w_norm / g_norm),\n",
        "                  1.0),\n",
        "              1.0)\n",
        "        scaled_lr = self.learning_rate * trust_ratio\n",
        "\n",
        "        next_v = tf.multiply(self.momentum, v) + scaled_lr * grad\n",
        "        if self.use_nesterov:\n",
        "          update = tf.multiply(self.momentum, next_v) + scaled_lr * grad\n",
        "        else:\n",
        "          update = next_v\n",
        "        next_param = param - update\n",
        "      else:\n",
        "        next_v = tf.multiply(self.momentum, v) + grad\n",
        "        if self.use_nesterov:\n",
        "          update = tf.multiply(self.momentum, next_v) + grad\n",
        "        else:\n",
        "          update = next_v\n",
        "\n",
        "        trust_ratio = 1.0\n",
        "        if self._do_layer_adaptation(param_name):\n",
        "          w_norm = tf.norm(param, ord=2)\n",
        "          v_norm = tf.norm(update, ord=2)\n",
        "          trust_ratio = tf.where(\n",
        "              tf.greater(w_norm, 0), tf.where(\n",
        "                  tf.greater(v_norm, 0), (self.eeta * w_norm / v_norm),\n",
        "                  1.0),\n",
        "              1.0)\n",
        "        scaled_lr = trust_ratio * self.learning_rate\n",
        "        next_param = param - scaled_lr * update\n",
        "\n",
        "      assignments.extend(\n",
        "          [param.assign(next_param),\n",
        "           v.assign(next_v),\n",
        "           global_step.assign(new_global_step)])\n",
        "    return tf.group(*assignments, name=name)\n",
        "\n",
        "  def _use_weight_decay(self, param_name):\n",
        "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "    if not self.weight_decay:\n",
        "      return False\n",
        "    if self.exclude_from_weight_decay:\n",
        "      for r in self.exclude_from_weight_decay:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True\n",
        "\n",
        "  def _do_layer_adaptation(self, param_name):\n",
        "    \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
        "    if self.exclude_from_layer_adaptation:\n",
        "      for r in self.exclude_from_layer_adaptation:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDCY4h7bHxj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d84980-115e-466c-cd5a-32f14b849e31"
      },
      "source": [
        "#@title Load tensorflow datasets: we use tensorflow flower dataset as an example\n",
        "features = {\n",
        "      'tree/genus/label':\n",
        "          tf.io.FixedLenFeature((), tf.int64),\n",
        "      'streetlevel/encoded': tf.io.FixedLenFeature((), tf.string),\n",
        "      'streetlevel/center/x': tf.io.FixedLenFeature((), tf.int64),\n",
        "      'streetlevel/center/y': tf.io.FixedLenFeature((), tf.int64),\n",
        "  }\n",
        "\n",
        "def get_paths(type, city, r=25):\n",
        "  pth = \"/content/drive/My Drive/computer_vision_project/data/\"+city+\"/\"\n",
        "  paths = [f\"{pth}{type}-000{i:02d}-of-00025\" for i in range(r)]\n",
        "  # images = []\n",
        "  # labels = []\n",
        "  # paths = []\n",
        "  # for i in range(r):\n",
        "  #   str_i = str(i) if i >= 10 else \"0\"+str(i)\n",
        "  #   filename = f'{type}-000{str_i}-of-00025'\n",
        "  #   path = pth+filename\n",
        "  #   paths.append(path)\n",
        "  return paths\n",
        "\n",
        "def _preprocess(record):\n",
        "  example = tf.io.parse_single_example(record, features)\n",
        "  ret = {}\n",
        "  ret[\"image\"] = tf.image.decode_jpeg(example['streetlevel/encoded'])\n",
        "  ret[\"label\"] = example['tree/genus/label']\n",
        "  ret[\"center_x\"] =  example['streetlevel/center/x']\n",
        "  ret[\"center_y\"] =  example['streetlevel/center/y']\n",
        "  height = 224 \n",
        "  width = 224 \n",
        "  ret['image'] = preprocess_image(ret['image'], height, width, (ret[\"center_x\"], ret[\"center_y\"]), is_training=True, color_distort=False)\n",
        "  return ret\n",
        "\n",
        "# freq_labels_tf = tf.constant([4, 303, 127, 15])\n",
        "# def predicate(x, allowed_labels=freq_labels_tf):\n",
        "#     label = x['label']\n",
        "#     isallowed = tf.not_equal(allowed_labels, tf.cast(label, allowed_labels.dtype))\n",
        "#     reduced = tf.reduce_sum(tf.cast(isallowed, tf.float32))\n",
        "#     return tf.greater(reduced, tf.constant(0.))\n",
        "\n",
        "def get_dataloaders(batch_size, num_train, num_val):\n",
        "    # Get paths to train and validation datasets\n",
        "    train_paths = get_paths(\"train\", \"kitchener\", r=num_train)\n",
        "    val_paths = get_paths(\"train\", \"kitchener\", r=num_train+num_val)[num_train:num_train+num_val]\n",
        "    \n",
        "    # Create train dataset\n",
        "    train_dataset = tf.data.TFRecordDataset(train_paths).map(_preprocess)\n",
        "    train_dataset = train_dataset.batch(batch_size)\n",
        "    \n",
        "    # Create validation dataset\n",
        "    val_dataset = tf.data.TFRecordDataset([val_paths]).map(_preprocess)\n",
        "    val_dataset = val_dataset.batch(batch_size)\n",
        "    \n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "paths = get_paths(\"train\", \"kitchener\", r=25)\n",
        "dataset, val_dataset = get_dataloaders(batch_size=5, num_train=4, num_val=1)\n",
        "x = tf.data.make_one_shot_iterator(dataset).get_next()\n",
        "\n",
        "num_images = 10\n",
        "num_classes = 344"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape\n",
            "Tensor(\"strided_slice:0\", shape=(), dtype=int32) Tensor(\"strided_slice_1:0\", shape=(), dtype=int32)\n",
            "Crop shape\n",
            "Tensor(\"cond/Identity:0\", shape=(), dtype=int32) Tensor(\"cond/Identity_1:0\", shape=(), dtype=int32)\n",
            "Offset shape\n",
            "Tensor(\"cond_3/Identity:0\", shape=(), dtype=int32) Tensor(\"cond_4/Identity:0\", shape=(), dtype=int32)\n",
            "Image shape\n",
            "Tensor(\"strided_slice:0\", shape=(), dtype=int32) Tensor(\"strided_slice_1:0\", shape=(), dtype=int32)\n",
            "Crop shape\n",
            "Tensor(\"cond/Identity:0\", shape=(), dtype=int32) Tensor(\"cond/Identity_1:0\", shape=(), dtype=int32)\n",
            "Offset shape\n",
            "Tensor(\"cond_3/Identity:0\", shape=(), dtype=int32) Tensor(\"cond_4/Identity:0\", shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WXspghpERRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1375e2-d5a6-4e31-cb49-54ed8a5a9fa8"
      },
      "source": [
        "#@title Load module and construct the computation graph\n",
        "\n",
        "# learning_rate = 0.1\n",
        "lr_schedule = [0.01, 0.01, 0.01, 0.01, 0.001]\n",
        "momentum = 0.9\n",
        "weight_decay = 0.\n",
        "\n",
        "# Load the base network and set it to non-trainable (for speedup fine-tuning)\n",
        "hub_path = 'gs://simclr-checkpoints/simclrv2/finetuned_100pct/r50_1x_sk0/hub/'\n",
        "module = hub.Module(hub_path, trainable=False)\n",
        "key = module(inputs=x['image'], signature=\"default\", as_dict=True)\n",
        "\n",
        "# Attach a trainable linear layer to adapt for the new task.\n",
        "with tf.variable_scope('head_supervised_new', reuse=tf.AUTO_REUSE):\n",
        "  logits_t = tf.layers.dense(inputs=key['final_avg_pool'], units=num_classes)\n",
        "loss_t = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "    labels=tf.one_hot(x['label'], num_classes), logits=logits_t))\n",
        "\n",
        "# Setup optimizer and training op.\n",
        "optimizer = LARSOptimizer(\n",
        "    lr_schedule[0],\n",
        "    momentum=momentum,\n",
        "    weight_decay=weight_decay,\n",
        "    exclude_from_weight_decay=['batch_normalization', 'bias', 'head_supervised'])\n",
        "variables_to_train = tf.trainable_variables() \n",
        "train_op = optimizer.minimize(\n",
        "    loss_t, global_step=tf.train.get_or_create_global_step(),\n",
        "    var_list=variables_to_train)\n",
        "\n",
        "print('Variables to train:', variables_to_train)\n",
        "key # The accessible tensor in the return dictionary"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variables to train: [<tf.Variable 'head_supervised_new/dense/kernel:0' shape=(2048, 344) dtype=float32>, <tf.Variable 'head_supervised_new/dense/bias:0' shape=(344,) dtype=float32>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4e912c8397c2>:15: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  logits_t = tf.layers.dense(inputs=key['final_avg_pool'], units=num_classes)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'initial_max_pool': <tf.Tensor 'module_apply_default/base_model/initial_max_pool:0' shape=(None, 56, 56, 64) dtype=float32>,\n",
              " 'block_group4': <tf.Tensor 'module_apply_default/base_model/block_group4:0' shape=(None, 7, 7, 2048) dtype=float32>,\n",
              " 'block_group3': <tf.Tensor 'module_apply_default/base_model/block_group3:0' shape=(None, 14, 14, 1024) dtype=float32>,\n",
              " 'final_avg_pool': <tf.Tensor 'module_apply_default/base_model/final_avg_pool:0' shape=(None, 2048) dtype=float32>,\n",
              " 'logits_sup': <tf.Tensor 'module_apply_default/head_supervised/linear_layer/linear_layer_out:0' shape=(None, 1000) dtype=float32>,\n",
              " 'block_group2': <tf.Tensor 'module_apply_default/base_model/block_group2:0' shape=(None, 28, 28, 512) dtype=float32>,\n",
              " 'initial_conv': <tf.Tensor 'module_apply_default/base_model/initial_conv:0' shape=(None, 112, 112, 64) dtype=float32>,\n",
              " 'block_group1': <tf.Tensor 'module_apply_default/base_model/block_group1:0' shape=(None, 56, 56, 256) dtype=float32>,\n",
              " 'default': <tf.Tensor 'module_apply_default/base_model/final_avg_pool:0' shape=(None, 2048) dtype=float32>}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title train and validation functions\n",
        "\n",
        "def train(sess, train_op, loss_t, x, lr_schedule, epoch, freq_labels, common_labels, num_images, batch_size):\n",
        "    epoch_loss = 0\n",
        "    epoch_correct = 0\n",
        "    epoch_total = 0\n",
        "    epoch_common_correct = 0\n",
        "    epoch_common_total = 0\n",
        "    epoch_freq_correct = 0\n",
        "    epoch_freq_total = 0\n",
        "\n",
        "    optimizer.learning_rate = lr_schedule[epoch]\n",
        "\n",
        "    batches_per_epoch = num_images // batch_size #one warmup at 0 learning rate\n",
        "\n",
        "    for batch in range(batches_per_epoch):\n",
        "        _, loss, image, logits, labels = sess.run((train_op, loss_t, x['image'], logits_t, x['label']))\n",
        "        pred = logits.argmax(-1)\n",
        "        freq = np.where(np.isin(labels,freq_labels))[0]\n",
        "        freq_correct = np.sum(pred[freq]==labels[freq])\n",
        "        freq_total = freq.size\n",
        "\n",
        "        common = np.where(np.isin(labels,common_labels))[0]\n",
        "        common_correct = np.sum(pred[common]==labels[common])\n",
        "        common_total = common.size\n",
        "\n",
        "        correct = np.sum(pred == labels)\n",
        "        total = labels.size\n",
        "\n",
        "        epoch_loss += loss\n",
        "        epoch_correct += correct\n",
        "        epoch_total += total\n",
        "        epoch_common_correct += common_correct\n",
        "        epoch_common_total += common_total\n",
        "        epoch_freq_correct += freq_correct\n",
        "        epoch_freq_total += freq_total\n",
        "\n",
        "    epoch_loss /= batches_per_epoch\n",
        "    epoch_accuracy = epoch_correct / float(epoch_total)\n",
        "    epoch_common_accuracy = epoch_common_correct / float(epoch_common_total)\n",
        "    epoch_freq_accuracy = epoch_freq_correct / float(epoch_freq_total)\n",
        "\n",
        "    return image, epoch_loss, epoch_accuracy, epoch_common_accuracy, epoch_freq_accuracy\n",
        "\n",
        "    # def evaluate(evaldata):\n",
        "\n",
        "    #   return epoch_accuracy, epoch_common_accuracy, epoch_freq_accuracy\n"
      ],
      "metadata": {
        "id": "S2EuEMMJY2kx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHj1FZ2dEXIj"
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num_images = 0\n",
        "# num_images = sess.run(dataset.reduce(0, lambda x, _: x + 1))\n",
        "# print(num_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6EzAE-3mvfy",
        "outputId": "d59c5304-dcf3-43d5-e3fe-d5c8d30c13eb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHSQuEwnCBKN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "aedcaf44-2b97-40c0-d134-f114c96020a3"
      },
      "source": [
        "#@title We fine-tune the new *linear layer* \n",
        "\n",
        "total_epochs = 2\n",
        "# batches_per_epoch = num_images // batch_size \n",
        "freq_labels = [4, 303, 127, 15]\n",
        "common_labels = [221, 246, 279, 313, 132, 247, 227, 235]\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "for epoch in range(total_epochs):\n",
        "  print(num_images)\n",
        "  image, epoch_loss, epoch_accuracy, epoch_common_accuracy, epoch_freq_accuracy = train(sess, train_op, loss_t, x, lr_schedule, epoch, freq_labels, common_labels, num_images, batch_size)\n",
        "  print(\"[Epoch {}] Loss: {:.4f} Top 1: {:.4f} Top 1 Common: {:.4f} Top 1 Freq: {:.4f}\".format(epoch+1, epoch_loss, epoch_accuracy, epoch_common_accuracy, epoch_freq_accuracy))\n",
        "  # # Save checkpoint\n",
        "  # saver.save(sess, '/path/to/save/checkpoint', global_step=epoch)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2770\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-bb60f61ffe8b>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_common_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_freq_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_schedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommon_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Epoch {}] Loss: {:.4f} Top 1: {:.4f} Top 1 Common: {:.4f} Top 1 Freq: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_common_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_freq_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# # Save checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-bb1ea66624b8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, train_op, loss_t, x, lr_schedule, epoch, freq_labels, common_labels, num_images, batch_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreq_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    969\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1192\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1372\u001b[0m                            run_metadata)\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1362\u001b[0m                                       target_list, run_metadata)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_session_run_lock\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5496\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_lock\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5497\u001b[0m     \"\"\"\n\u001b[0;32m-> 5498\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SESSION_RUN_LOCK_GROUP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/lock_util.py\u001b[0m in \u001b[0;36mgroup\u001b[0;34m(self, group_id)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \"\"\"\n\u001b[1;32m     82\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_group_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOhK8anzK21K"
      },
      "source": [
        "# #@title Plot the images and predictions\n",
        "# fig, axes = plt.subplots(5, 1, figsize=(15, 15))\n",
        "# for i in range(5):\n",
        "#   axes[i].imshow(image[i])\n",
        "#   true_text = labels[i]\n",
        "#   pred_text = pred[i]\n",
        "#   axes[i].axis('off')\n",
        "#   axes[i].text(288, 192, 'Truth: ' + str(true_text) + '\\n' + 'Pred: ' + str(pred_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4/28\n",
        "[Iter 1] Loss: 5.400359630584717 Top 1: 0.12\n",
        "[Iter 2] Loss: 4.544557094573975 Top 1: 0.16\n",
        "[Iter 3] Loss: 3.9050605297088623 Top 1: 0.32\n",
        "[Iter 4] Loss: 4.171600818634033 Top 1: 0.24\n",
        "[Iter 5] Loss: 3.9627764225006104 Top 1: 0.08\n",
        "[Iter 6] Loss: 3.4366681575775146 Top 1: 0.16\n",
        "[Iter 7] Loss: 4.415002822875977 Top 1: 0.12\n",
        "[Iter 8] Loss: 4.020637035369873 Top 1: 0.16\n",
        "[Iter 9] Loss: 4.018008708953857 Top 1: 0.04\n",
        "[Iter 10] Loss: 2.8650598526000977 Top 1: 0.16"
      ],
      "metadata": {
        "id": "QqrmIH99_5O1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4/28 (batch size 64) \n",
        "[Iter 1] Loss: 5.980161666870117 Top 1: 0.0\n",
        "[Iter 2] Loss: 5.287200927734375 Top 1: 0.296875\n",
        "[Iter 3] Loss: 4.178678512573242 Top 1: 0.25\n",
        "[Iter 4] Loss: 5.221988677978516 Top 1: 0.171875\n",
        "[Iter 5] Loss: 4.716645240783691 Top 1: 0.171875\n",
        "[Iter 6] Loss: 4.0522308349609375 Top 1: 0.078125\n",
        "[Iter 7] Loss: 3.888197422027588 Top 1: 0.078125\n",
        "[Iter 8] Loss: 3.840484619140625 Top 1: 0.078125\n",
        "[Iter 9] Loss: 4.675423622131348 Top 1: 0.203125\n",
        "[Iter 10] Loss: 4.1054511070251465 Top 1: 0.265625"
      ],
      "metadata": {
        "id": "OY8ltHk2KOQL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PRHFa_dSqPxj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}