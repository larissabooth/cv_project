{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larissabooth/cv_project/blob/main/evaluate_ss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "mqVM6wqc3k3-",
        "outputId": "962e61a2-ba51-4187-c600-5060258e2c4a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_36062/1408506528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSwD6err3tMt"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "\n",
        "# All rights reserved.\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import signal\n",
        "import sys\n",
        "import time\n",
        "import urllib\n",
        "\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# default `log_dir` is \"runs\" - more specific here\n",
        "# writer = SummaryWriter('runs/FT_exp_3')\n",
        "\n",
        "sys.path.append(\"/content/drive/My Drive/computer_vision_project/vicreg\")\n",
        "\n",
        "import resnet\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "IqF1zEYB4rGY",
        "outputId": "8f84506b-98c5-43d8-eefa-c0e320c60103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/My Drive/computer_vision_project/vicreg'\n",
            "/home/lara\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_36062/3262496229.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/computer_vision_project/Kitchener_torch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtrain_percent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.1\u001b[0m \u001b[0;31m#\"size of traing set in percent\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtest_percent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
          ]
        }
      ],
      "source": [
        "#@title Network configurations\n",
        "\n",
        "%cd \"/content/drive/My Drive/computer_vision_project/vicreg\"\n",
        "\n",
        "from datetime import datetime\n",
        "currentDateAndTime = datetime.now()\n",
        "\n",
        "currentTime = currentDateAndTime.strftime(\"%H_%M\")\n",
        "\n",
        "#Data\n",
        "data_dir = Path(\"/content/drive/My Drive/computer_vision_project/Kitchener_torch\")\n",
        "train_percent = .1 #\"size of traing set in percent\"\n",
        "test_percent = 1.0\n",
        "\n",
        "# Checkpoint\n",
        "ckpt_file = Path(\"checkpoint_\"+currentTime+\".pth\")\n",
        "pretrained = \"./checkpoints/self_sup/500_epochs/resnet50_bb.pth\" #path to pretrained model\n",
        "exp_dir = Path(\"./checkpoints/ss_lincls\") #path to export directory\n",
        "print_freq = 5 #number of steps before printing\n",
        "\n",
        "# Model\n",
        "arch = \"resnet50\"\n",
        "\n",
        "# Optim\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "lr_backbone = 0.0 #\"backbone base learning rate\"\n",
        "lr_head = 0.01 #\"classifier base learning rate\"\n",
        "weight_decay = 0 #1e-6\n",
        "weights = \"freeze\" #(\"finetune\", \"freeze\")\n",
        "\n",
        "# Running\n",
        "workers= 8 #\"number of data loader workers\"\n",
        "rank = 0\n",
        "\n",
        "#Stats file\n",
        "train_stats_file =open(\"./stats/ss_lincls/train_stats_lr_\"+str(lr_head)+\"_\"+currentTime+\".json\", \"a\", buffering=1)\n",
        "val_stats_file =open(\"./stats/ss_lincls/val_stats_lr_\"+str(lr_head)+\"_\"+currentTime+\".json\", \"a\", buffering=1)\n",
        "test_stats_file =open(\"./stats/ss_lincls/test_stats_lr_\"+str(lr_head)+\"_\"+currentTime+\".json\", \"a\", buffering=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bieTW6P1HJWH"
      },
      "outputs": [],
      "source": [
        "#@title Accuracy helpers\n",
        "\n",
        "import argparse\n",
        "\n",
        "def calc_accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self, name, fmt=\":f\"):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
        "        return fmtstr.format(**self.__dict__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikgI-MTTk8o0"
      },
      "outputs": [],
      "source": [
        "#@title Class names\n",
        "classes = (\n",
        "\"acer\",\n",
        "\"tilia\",\n",
        "\"fraxinus\", \n",
        "\"ulmus\", \n",
        "\"sorbus\", \n",
        "\"amelanchier\", \n",
        "\"crataegus\", \n",
        "\"gleditsia\", \n",
        "\"prunus\", \n",
        "\"quercus\", \n",
        "\"picea\", \n",
        "\"populus\", \n",
        "\"platanus\", \n",
        "\"pyrus\", \n",
        "\"salix\", \n",
        "\"fagus\", \n",
        "\"gymnocladus\", \n",
        "\"cornus\", \n",
        "\"betula\", \n",
        "\"taxus\", \n",
        "\"ginkgo\", \n",
        "\"magnolia\", \n",
        "\"pinus\", \n",
        "\"aesculus\", \n",
        "\"abies\", \n",
        "\"corylus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-Pa58iE3c9I"
      },
      "outputs": [],
      "source": [
        "#@title Load model\n",
        "\n",
        "backbone, embedding = resnet.__dict__[arch](zero_init_residual=True)\n",
        "state_dict = torch.load(pretrained, map_location=\"cpu\")\n",
        "if \"model\" in state_dict:\n",
        "    state_dict = state_dict[\"model\"]\n",
        "    state_dict = {\n",
        "        key.replace(\"backbone.\", \"\"): value\n",
        "        for (key, value) in state_dict.items()\n",
        "    }\n",
        "backbone.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "head = nn.Linear(embedding, 26)\n",
        "head.weight.data.normal_(mean=0.0, std=0.01)\n",
        "head.bias.data.zero_()\n",
        "model = nn.Sequential(backbone, head)\n",
        "model.to(device)\n",
        "\n",
        "if weights == \"freeze\":\n",
        "    backbone.requires_grad_(False)\n",
        "    head.requires_grad_(True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "param_groups = [dict(params=head.parameters(), lr=lr_head)]\n",
        "if weights == \"finetune\":\n",
        "    param_groups.append(dict(params=backbone.parameters(), lr=lr_backbone))\n",
        "optimizer = optim.SGD(param_groups, 0, momentum=0.9, weight_decay=weight_decay)\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
        "\n",
        "# automatically resume from checkpoint if it exists\n",
        "if (exp_dir / ckpt_file).is_file():\n",
        "    ckpt = torch.load(exp_dir / ckpt_file, map_location=\"cpu\")\n",
        "    start_epoch = ckpt[\"epoch\"]\n",
        "    best_acc = ckpt[\"best_acc\"]\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "    # scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
        "else:\n",
        "    start_epoch = 0\n",
        "    best_acc = argparse.Namespace(top1=0, top5=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH_MskXlA5DG"
      },
      "outputs": [],
      "source": [
        "#@title Data loading code\n",
        "from torch.utils.data import random_split\n",
        "traindir = data_dir / \"train\"\n",
        "valdir = data_dir / \"val\"\n",
        "testdir = data_dir / \"test\"\n",
        "\n",
        "normalize = transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "\n",
        "#training data\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    traindir,\n",
        "    transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomResizedCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "actual_train_dataset, val_dataset, extra = random_split(train_dataset, [train_percent, train_percent, 1.0-train_percent*2])\n",
        "\n",
        "\n",
        "\n",
        "# testing data\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    testdir,\n",
        "    transforms.Compose(\n",
        "        [\n",
        "            # transforms.Resize(256),\n",
        "            # transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "actual_test_dataset, spare = random_split(test_dataset, [test_percent, 1.0 - test_percent])\n",
        "\n",
        "kwargs = dict(\n",
        "    batch_size=batch_size,\n",
        "    num_workers=workers,\n",
        "    shuffle = True,\n",
        "    pin_memory = True\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(actual_train_dataset,  **kwargs)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(actual_test_dataset, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Count the number of images per label\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "label_counts = Counter(train_dataset.targets)\n",
        "\n",
        "# Find the frequent, common, and rare labels\n",
        "freq_labels = []\n",
        "common_labels = []\n",
        "rare_labels = []\n",
        "\n",
        "for label, count in label_counts.items():\n",
        "    if count > 1000:\n",
        "        freq_labels.append(label)\n",
        "    elif count > 100:\n",
        "        common_labels.append(label)\n",
        "    else:\n",
        "        rare_labels.append(label)\n",
        "\n",
        "# Print the results\n",
        "print(\"Frequent labels:\", freq_labels)\n",
        "print(\"Common labels:\", common_labels)\n",
        "print(\"Rare labels:\", rare_labels)"
      ],
      "metadata": {
        "id": "7Y3iIrICcD9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# subset_classes = [0, 1, 2]\n",
        "# preds = np.array([0, 1, 1, 0, 2, 1, 2, 0, 0, 1, 3, 4])\n",
        "# labels =np.array([0, 1, 2, 0, 2, 1, 2, 0, 0, 1, 5, 5])\n",
        "# num_classes = 6\n",
        "def get_class_sums(preds, labels, num_classes):\n",
        "  numer = np.zeros(num_classes)\n",
        "  denom = np.zeros(num_classes)\n",
        "  correct = (preds == labels)\n",
        "  pred_unique , pred_counts = np.unique(labels[correct], return_counts=True)\n",
        "  unique , counts = np.unique(labels, return_counts=True)\n",
        "  numer[pred_unique] = pred_counts\n",
        "  denom[unique] = counts\n",
        "  return numer, denom\n",
        "# numer, denom = get_class_sums(preds, labels, num_classes)\n",
        "def get_class_ave_recall(classes, numer, denom):\n",
        "  class_numer = numer[classes]\n",
        "  class_denom = denom[classes]\n",
        "  nonzero_denom = np.nonzero(class_denom)\n",
        "  print(\"class_numer\", class_numer, \"class_denom\", class_denom, \"nonzero\", nonzero_denom)\n",
        "  if nonzero_denom[0].size == 0:\n",
        "    return -1.0\n",
        "  class_numer = class_numer[nonzero_denom]\n",
        "  class_denom = class_denom[nonzero_denom]\n",
        "  summed = np.sum(np.divide(class_numer, class_denom))\n",
        "  return summed/class_denom.size\n",
        "# print(get_class_ave_recall(subset_classes, numer, denom)) # answer should be 0.88888"
      ],
      "metadata": {
        "id": "AqVreEz1atyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBg0Kwbg-_Hh"
      },
      "outputs": [],
      "source": [
        "#@title training code\n",
        "NUM_CLASSES = 26\n",
        "def train():\n",
        "    start_time = time.time()\n",
        "\n",
        "    #total\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        # train\n",
        "        if weights == \"finetune\":\n",
        "            model.train()\n",
        "        elif weights == \"freeze\":\n",
        "            model.eval()\n",
        "        else:\n",
        "            assert False\n",
        "        print(\"Starting epoch:\", epoch)\n",
        "        for step, (images, target) in enumerate(\n",
        "            train_loader, start=epoch * len(train_loader)\n",
        "        ):\n",
        "            output = model(images.to(device))\n",
        "            _, preds = torch.max(output, 1)\n",
        "\n",
        "            loss = criterion(output, target.to(device))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #ground truth and predictions\n",
        "            labels = target.cpu().numpy()\n",
        "            preds = preds.cpu().numpy()\n",
        "\n",
        "            #total\n",
        "            correct += np.sum(preds == labels)\n",
        "            total += labels.size\n",
        "            \n",
        "            #compute recall & accuracy\n",
        "            \n",
        "            if step % print_freq == 0:\n",
        "                pg = optimizer.param_groups\n",
        "                lr_head = pg[0][\"lr\"]\n",
        "                lr_backbone = pg[1][\"lr\"] if len(pg) == 2 else 0\n",
        "\n",
        "                accuracy = correct / float(total)\n",
        "                #record current stats\n",
        "                stats = dict(\n",
        "                    epoch=epoch,\n",
        "                    step=step,\n",
        "                    lr_backbone=lr_backbone,\n",
        "                    lr_head=lr_head,\n",
        "                    loss=loss.item,\n",
        "                    accuracy=accuracy*100,\n",
        "                    time=int(time.time() - start_time),\n",
        "                )\n",
        "\n",
        "                #total\n",
        "                correct = 0.0\n",
        "                total = 0.0\n",
        "\n",
        "                # print(json.dumps(stats))\n",
        "                print(json.dumps(stats), file=train_stats_file)\n",
        "        print(\"finished training epoch\")\n",
        "\n",
        "\n",
        "        # evaluate\n",
        "        model.eval()\n",
        "        if rank == 0:\n",
        "            top1 = AverageMeter(\"Acc@1\")\n",
        "            top5 = AverageMeter(\"Acc@5\")\n",
        "            with torch.no_grad():\n",
        "                sum_numer = np.zeros(NUM_CLASSES)\n",
        "                sum_denom = np.zeros(NUM_CLASSES)\n",
        "\n",
        "                for images, target in val_loader:\n",
        "                    output = model(images.to(device))\n",
        "                    acc1, acc5 = calc_accuracy(\n",
        "                        output, target.to(device), topk=(1, 5)\n",
        "                    )\n",
        "                    top1.update(acc1[0].item(), images.size(0))\n",
        "                    top5.update(acc5[0].item(), images.size(0))\n",
        "                    _, preds = torch.max(output, 1)\n",
        "                    preds = preds.cpu().numpy()\n",
        "\n",
        "                    labels = target.cpu().numpy()\n",
        "                    loss = criterion(output, target.to(device))\n",
        "\n",
        "                    numer, denom = get_class_sums(preds, labels, NUM_CLASSES)\n",
        "\n",
        "                    sum_numer += numer\n",
        "                    sum_denom += denom\n",
        "\n",
        "            best_acc.top1 = max(best_acc.top1, top1.avg)\n",
        "            best_acc.top5 = max(best_acc.top5, top5.avg)\n",
        "            \n",
        "            #compute recalls\n",
        "            freq_recall = get_class_ave_recall(freq_labels, sum_numer, sum_denom)\n",
        "            common_recall = get_class_ave_recall(common_labels, sum_numer, sum_denom)\n",
        "            rare_recall = get_class_ave_recall(rare_labels, sum_numer, sum_denom)\n",
        "            class_avg_recall = get_class_ave_recall([i for i in range(NUM_CLASSES)], sum_numer, sum_denom)\n",
        "\n",
        "            stats = dict(\n",
        "                epoch=epoch,\n",
        "                acc1=top1.avg,\n",
        "                acc5=top5.avg,\n",
        "                loss=loss.item(),\n",
        "                freq_recall=freq_recall,\n",
        "                common_recall=common_recall,\n",
        "                rare_recall=rare_recall,\n",
        "                class_avg_recall=class_avg_recall,\n",
        "                best_acc1=best_acc.top1,\n",
        "                best_acc5=best_acc.top5,\n",
        "            )\n",
        "            # print(\"val_stats\", stats)\n",
        "            print(json.dumps(stats))\n",
        "            print(json.dumps(stats), file=val_stats_file)\n",
        "        # scheduler.step()\n",
        "        if rank == 0:\n",
        "            state = dict(\n",
        "                epoch=epoch + 1,\n",
        "                best_acc=best_acc,\n",
        "                model=model.state_dict(),\n",
        "                optimizer=optimizer.state_dict(),\n",
        "                # scheduler=scheduler.state_dict(),\n",
        "            )\n",
        "            torch.save(state, exp_dir / ckpt_file)\n",
        "        print(\"finishing STEP\", step)\n",
        "\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "\n",
        "# All rights reserved.\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title testing code \n",
        "\n",
        "def test():\n",
        "        # evaluate\n",
        "        model.eval()\n",
        "        if rank == 0:\n",
        "            top1_test = AverageMeter(\"Acc@1\")\n",
        "            top5_test = AverageMeter(\"Acc@5\")\n",
        "            with torch.no_grad():\n",
        "                sum_numer = np.zeros(NUM_CLASSES)\n",
        "                sum_denom = np.zeros(NUM_CLASSES)\n",
        "                for images, target in test_loader:\n",
        "                    output = model(images.to(device))\n",
        "                    acc1, acc5 = calc_accuracy(\n",
        "                        output, target.to(device), topk=(1, 5)\n",
        "                    )\n",
        "                    top1_test.update(acc1[0].item(), images.size(0))\n",
        "                    top5_test.update(acc5[0].item(), images.size(0))\n",
        "                    _, preds = torch.max(output, 1)\n",
        "                    preds = preds.cpu().numpy()\n",
        "\n",
        "                    labels = target.cpu().numpy()\n",
        "\n",
        "                    numer, denom = get_class_sums(preds, labels, NUM_CLASSES)\n",
        "\n",
        "                    sum_numer += numer\n",
        "                    sum_denom += denom\n",
        "            \n",
        "            #compute recalls\n",
        "            freq_recall = get_class_ave_recall(freq_labels, sum_numer, sum_denom)\n",
        "            common_recall = get_class_ave_recall(common_labels, sum_numer, sum_denom)\n",
        "            rare_recall = get_class_ave_recall(rare_labels, sum_numer, sum_denom)\n",
        "            class_avg_recall = get_class_ave_recall([i for i in range(NUM_CLASSES)], sum_numer, sum_denom)\n",
        "\n",
        "            stats = dict(\n",
        "                acc1=top1_test.avg,\n",
        "                acc5=top5_test.avg,\n",
        "                freq_recall=freq_recall,\n",
        "                common_recall=common_recall,\n",
        "                rare_recall=rare_recall,\n",
        "                class_avg_recall=class_avg_recall\n",
        "            )\n",
        "            # print(\"test_stats\", stats)\n",
        "            print(json.dumps(stats))\n",
        "            print(json.dumps(stats), file=test_stats_file)"
      ],
      "metadata": {
        "id": "hUsvR8PDI7wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YT2RadrHnLW"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "id": "L7LPQmlJQ6pP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}