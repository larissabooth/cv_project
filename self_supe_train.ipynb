{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larissabooth/cv_project/blob/main/self_supe_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBSJ1iw6Gmoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d0c8f64-d03a-40ad-d4c5-0a780304d4e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports \n",
        "\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "\n",
        "# All rights reserved.\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import signal\n",
        "import sys\n",
        "import time\n",
        "import urllib\n",
        "\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "import torch.distributed as dist\n",
        "\n",
        "sys.path.append(\"/content/drive/My Drive/computer_vision_project/vicreg\")\n",
        "\n",
        "import augmentations as aug\n",
        "# from distributed import init_distributed_mode\n",
        "\n",
        "import resnet\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "torch.backends.cudnn.benchmark = True\n",
        "gpu = torch.device(device)\n"
      ],
      "metadata": {
        "id": "XpzTmmpCGwM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Network configurations\n",
        "\n",
        "%cd \"/content/drive/My Drive/computer_vision_project/vicreg\"\n",
        "\n",
        "#time tools\n",
        "from datetime import datetime\n",
        "currentDateAndTime = datetime.now()\n",
        "\n",
        "currentTime = currentDateAndTime.strftime(\"%H_%M\")\n",
        "\n",
        "#Data\n",
        "data_dir = Path(\"/content/drive/My Drive/computer_vision_project/Kitchener_torch\")\n",
        "\n",
        "# Checkpoint\n",
        "ckpt_file = Path(\"checkpoint.pth\")\n",
        "pretrained = Path(\"./checkpoints/lincls/resnet50_fullckpt.pth\") #path to pretrained model\n",
        "exp_dir = Path(\"./checkpoints/self_sup\") #path to export directory\n",
        "log_freq_time = 10 #'Print logs to the stats.txt file every [log-freq-time] seconds'\n",
        "\n",
        "# Model\n",
        "arch = \"resnet50\"\n",
        "mlp = \"8192-8192-8192\" #'Size and number of layers of the MLP expander head'\n",
        "\n",
        "# Optim\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "base_lr = 0.1 #\"Base learning rate, effective learning after warmup is [base-lr] * [batch-size] / 256\n",
        "wd = 0 #weight decay\n",
        "\n",
        "#Loss\n",
        "sim_coeff = 25.0 #'Invariance regularization loss coefficient'\n",
        "std_coeff = 25.0 #'Variance regularization loss coefficient'\n",
        "cov_coeff = 1.0  #'Covariance regularization loss coefficient'\n",
        "\n",
        "# Running\n",
        "num_workers= 4 #\"number of data loader workers\"\n",
        "\n",
        "#Stats file\n",
        "# stats_file =open(\"./stats/self_sup/stats_file_\"+currentTime+\".json\", \"a\", buffering=1)\n",
        "\n",
        "exp_dir.mkdir(parents=True, exist_ok=True)\n",
        "stats_file = open(\"./stats/self_sup/stats_file_\"+currentTime+\".txt\", \"a\", buffering=1)\n",
        "print(\" \".join(sys.argv))\n",
        "print(\" \".join(sys.argv), file=stats_file)\n"
      ],
      "metadata": {
        "id": "9Tld61JqG1Kf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41fcf454-dd6a-4950-c731-6721ba839965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1RST5HayuSWDl47eVsz8vHQpNYKmjc800/computer_vision_project/vicreg\n",
            "/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py -f /root/.local/share/jupyter/runtime/kernel-c0a9332a-ff12-4f80-bcbf-e081d223deb4.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FullGatherLayer(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Gather tensors from all process and support backward propagation\n",
        "    for the gradients across processes.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        output = [torch.zeros_like(x) for _ in range(dist.get_world_size())]\n",
        "        dist.all_gather(output, x)\n",
        "        return tuple(output)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, *grads):\n",
        "        all_gradients = torch.stack(grads)\n",
        "        dist.all_reduce(all_gradients)\n",
        "        return all_gradients[dist.get_rank()]"
      ],
      "metadata": {
        "id": "hojqTzgUHpTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title VICReg and Projector\n",
        "\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, mlp, arch, batch_size, sim_coeff, std_coeff, cov_coeff):\n",
        "        super().__init__()\n",
        "        self.mlp = mlp\n",
        "        self.arch = arch\n",
        "        self.batch_size = batch_size\n",
        "        self.sim_coeff = sim_coeff\n",
        "        self.std_coeff = std_coeff\n",
        "        self.cov_coeff = cov_coeff\n",
        "        self.num_features = int(mlp.split(\"-\")[-1])\n",
        "        self.backbone, self.embedding = resnet.__dict__[arch](\n",
        "            zero_init_residual=True\n",
        "        )\n",
        "        self.projector = Projector(mlp, self.embedding)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = self.projector(self.backbone(x))\n",
        "        y = self.projector(self.backbone(y))\n",
        "\n",
        "        repr_loss = F.mse_loss(x, y)\n",
        "\n",
        "        # x = torch.cat(FullGatherLayer.apply(x), dim=0)\n",
        "        # y = torch.cat(FullGatherLayer.apply(y), dim=0)\n",
        "        x = torch.cat([x], dim=0)\n",
        "        y = torch.cat([y], dim=0)\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        cov_x = (x.T @ x) / (self.batch_size - 1)\n",
        "        cov_y = (y.T @ y) / (self.batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(\n",
        "            self.num_features\n",
        "        ) + off_diagonal(cov_y).pow_(2).sum().div(self.num_features)\n",
        "\n",
        "        loss = (\n",
        "            self.sim_coeff * repr_loss\n",
        "            + self.std_coeff * std_loss\n",
        "            + self.cov_coeff * cov_loss\n",
        "        )\n",
        "        return loss\n",
        "\n",
        "\n",
        "def Projector(mlp, embedding):\n",
        "    mlp_spec = f\"{embedding}-{mlp}\"\n",
        "    layers = []\n",
        "    f = list(map(int, mlp_spec.split(\"-\")))\n",
        "    for i in range(len(f) - 2):\n",
        "        layers.append(nn.Linear(f[i], f[i + 1]))\n",
        "        layers.append(nn.BatchNorm1d(f[i + 1]))\n",
        "        layers.append(nn.ReLU(True))\n",
        "    layers.append(nn.Linear(f[-2], f[-1], bias=False))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def exclude_bias_and_norm(p):\n",
        "    return p.ndim == 1\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()"
      ],
      "metadata": {
        "id": "Gbk-5JZWwDdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LARS Optimizer and Adjust Learning Rate\n",
        "\n",
        "class LARS(optim.Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params,\n",
        "        lr,\n",
        "        weight_decay=0,\n",
        "        momentum=0.9,\n",
        "        eta=0.001,\n",
        "        weight_decay_filter=None,\n",
        "        lars_adaptation_filter=None,\n",
        "    ):\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay,\n",
        "            momentum=momentum,\n",
        "            eta=eta,\n",
        "            weight_decay_filter=weight_decay_filter,\n",
        "            lars_adaptation_filter=lars_adaptation_filter,\n",
        "        )\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for g in self.param_groups:\n",
        "            for p in g[\"params\"]:\n",
        "                dp = p.grad\n",
        "\n",
        "                if dp is None:\n",
        "                    continue\n",
        "\n",
        "                if g[\"weight_decay_filter\"] is None or not g[\"weight_decay_filter\"](p):\n",
        "                    dp = dp.add(p, alpha=g[\"weight_decay\"])\n",
        "\n",
        "                if g[\"lars_adaptation_filter\"] is None or not g[\n",
        "                    \"lars_adaptation_filter\"\n",
        "                ](p):\n",
        "                    param_norm = torch.norm(p)\n",
        "                    update_norm = torch.norm(dp)\n",
        "                    one = torch.ones_like(param_norm)\n",
        "                    q = torch.where(\n",
        "                        param_norm > 0.0,\n",
        "                        torch.where(\n",
        "                            update_norm > 0, (g[\"eta\"] * param_norm / update_norm), one\n",
        "                        ),\n",
        "                        one,\n",
        "                    )\n",
        "                    dp = dp.mul(q)\n",
        "\n",
        "                param_state = self.state[p]\n",
        "                if \"mu\" not in param_state:\n",
        "                    param_state[\"mu\"] = torch.zeros_like(p)\n",
        "                mu = param_state[\"mu\"]\n",
        "                mu.mul_(g[\"momentum\"]).add_(dp)\n",
        "\n",
        "                p.add_(mu, alpha=-g[\"lr\"])\n",
        "\n",
        "def adjust_learning_rate(optimizer, loader, step):\n",
        "    max_steps = epochs * len(loader)\n",
        "    warmup_steps = 10 * len(loader)\n",
        "    base_lr = base_lr * batch_size / 256\n",
        "    if step < warmup_steps:\n",
        "        lr = base_lr * step / warmup_steps\n",
        "    else:\n",
        "        step -= warmup_steps\n",
        "        max_steps -= warmup_steps\n",
        "        q = 0.5 * (1 + math.cos(math.pi * step / max_steps))\n",
        "        end_lr = base_lr * 0.001\n",
        "        lr = base_lr * q + end_lr * (1 - q)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "    return lr"
      ],
      "metadata": {
        "id": "GD5tdr45wt-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataloader\n",
        "from torch.utils.data import random_split\n",
        "transforms = aug.TrainTransform()\n",
        "\n",
        "train_dataset = datasets.ImageFolder(data_dir / \"train\", transforms)\n",
        "\n",
        "#very small batch for testing!\n",
        "dataset, extra = random_split(train_dataset, [0.1, 0.9])\n",
        "loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "XnxqQwLm-TLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model config\n",
        "\n",
        "model = VICReg(mlp, arch, batch_size, sim_coeff, std_coeff, cov_coeff).cuda(gpu)\n",
        "model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
        "# model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])\n",
        "optimizer = LARS(\n",
        "    model.parameters(),\n",
        "    lr=0,\n",
        "    weight_decay=wd,\n",
        "    weight_decay_filter=exclude_bias_and_norm,\n",
        "    lars_adaptation_filter=exclude_bias_and_norm,\n",
        ")\n",
        "\n",
        "if (pretrained).is_file():\n",
        "    print(\"resuming from checkpoint\")\n",
        "    # ckpt = torch.load(pretrained, map_location=\"cpu\")\n",
        "    # start_epoch = ckpt[\"epoch\"]\n",
        "    # model.load_state_dict(ckpt[\"model\"])\n",
        "    # optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "    checkpoint = torch.load(pretrained, map_location='cpu')\n",
        "    model_state_dict = {}\n",
        "    opt_state_dict = {}\n",
        "    for key in checkpoint['model']:\n",
        "        # map the keys in the state dictionary to the current model\n",
        "        new_key = key.replace(\"module.\", \"\")\n",
        "        model_state_dict[new_key] = checkpoint['model'][key]\n",
        "    for key in checkpoint['optimizer']:\n",
        "        # map the keys in the state dictionary to the current model\n",
        "        new_key = key.replace(\"module.\", \"\")\n",
        "        opt_state_dict[new_key] = checkpoint['optimizer'][key]\n",
        "\n",
        "    start_epoch = checkpoint[\"epoch\"]\n",
        "    model.load_state_dict(model_state_dict)\n",
        "    optimizer.load_state_dict(opt_state_dict)\n",
        "else:\n",
        "    start_epoch = 0"
      ],
      "metadata": {
        "id": "IRAFaXCE-hP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5d4c0c-cfb3-4323-f897-c22618761783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resuming from checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title train sequence \n",
        "def train():\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    start_time = last_logging = time.time()\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    for epoch in range(start_epoch, start_epoch+epochs):\n",
        "        for step, ((x, y), _) in enumerate(loader, start=epoch * len(loader)):\n",
        "            x = x.cuda(gpu, non_blocking=True)\n",
        "            y = y.cuda(gpu, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                loss = model.forward(x, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            current_time = time.time()\n",
        "            if current_time - last_logging > log_freq_time:\n",
        "                stats = dict(\n",
        "                  epoch=epoch,\n",
        "                    step=step,\n",
        "                    loss=loss.item(),\n",
        "                    time=int(current_time - start_time),\n",
        "                    lr=lr,\n",
        "                )\n",
        "                print(json.dumps(stats))\n",
        "                print(json.dumps(stats), file=stats_file)\n",
        "                last_logging = current_time\n",
        "\n",
        "        state = dict(\n",
        "            epoch=epoch + 1,\n",
        "            model=model.state_dict(),\n",
        "            optimizer=optimizer.state_dict(),\n",
        "        )\n",
        "        #save the full checkpoint\n",
        "        torch.save(state, exp_dir / \"resnet50_full.pth\")\n",
        "\n",
        "    #save just the backbone\n",
        "    torch.save(model.backbone.state_dict(), exp_dir / \"resnet50_bb.pth\")\n"
      ],
      "metadata": {
        "id": "sWhvxQv-G2z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "DKhZ4pbqAEZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef17c14a-f372-458d-e4cd-d853d0b4fc52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"epoch\": 1000, \"step\": 11000, \"loss\": 23.628154754638672, \"time\": 106, \"lr\": 0.0032}\n",
            "{\"epoch\": 1000, \"step\": 11004, \"loss\": 23.59101104736328, \"time\": 186, \"lr\": 0.0032}\n",
            "{\"epoch\": 1000, \"step\": 11008, \"loss\": 23.7258358001709, \"time\": 280, \"lr\": 0.0032}\n",
            "{\"epoch\": 1001, \"step\": 11011, \"loss\": 23.64259147644043, \"time\": 298, \"lr\": 0.0032}\n",
            "{\"epoch\": 1001, \"step\": 11021, \"loss\": 23.61747169494629, \"time\": 309, \"lr\": 0.0032}\n",
            "{\"epoch\": 1002, \"step\": 11022, \"loss\": 23.50779151916504, \"time\": 319, \"lr\": 0.0032}\n",
            "{\"epoch\": 1002, \"step\": 11031, \"loss\": 23.55649185180664, \"time\": 329, \"lr\": 0.0032}\n",
            "{\"epoch\": 1003, \"step\": 11033, \"loss\": 23.586668014526367, \"time\": 340, \"lr\": 0.0032}\n",
            "{\"epoch\": 1003, \"step\": 11042, \"loss\": 23.505640029907227, \"time\": 350, \"lr\": 0.0032}\n",
            "{\"epoch\": 1004, \"step\": 11044, \"loss\": 23.455398559570312, \"time\": 362, \"lr\": 0.0032}\n",
            "{\"epoch\": 1004, \"step\": 11053, \"loss\": 23.413198471069336, \"time\": 372, \"lr\": 0.0032}\n",
            "{\"epoch\": 1005, \"step\": 11055, \"loss\": 23.2421817779541, \"time\": 383, \"lr\": 0.0032}\n",
            "{\"epoch\": 1005, \"step\": 11065, \"loss\": 23.240217208862305, \"time\": 394, \"lr\": 0.0032}\n",
            "{\"epoch\": 1006, \"step\": 11066, \"loss\": 23.311084747314453, \"time\": 405, \"lr\": 0.0032}\n",
            "{\"epoch\": 1006, \"step\": 11076, \"loss\": 23.341135025024414, \"time\": 416, \"lr\": 0.0032}\n",
            "{\"epoch\": 1007, \"step\": 11077, \"loss\": 23.166885375976562, \"time\": 426, \"lr\": 0.0032}\n",
            "{\"epoch\": 1007, \"step\": 11086, \"loss\": 23.140159606933594, \"time\": 436, \"lr\": 0.0032}\n",
            "{\"epoch\": 1008, \"step\": 11088, \"loss\": 23.174177169799805, \"time\": 447, \"lr\": 0.0032}\n",
            "{\"epoch\": 1008, \"step\": 11097, \"loss\": 22.99793815612793, \"time\": 457, \"lr\": 0.0032}\n",
            "{\"epoch\": 1009, \"step\": 11099, \"loss\": 23.091251373291016, \"time\": 469, \"lr\": 0.0032}\n",
            "{\"epoch\": 1009, \"step\": 11109, \"loss\": 23.005573272705078, \"time\": 480, \"lr\": 0.0032}\n",
            "{\"epoch\": 1010, \"step\": 11110, \"loss\": 22.957456588745117, \"time\": 491, \"lr\": 0.0032}\n",
            "{\"epoch\": 1010, \"step\": 11119, \"loss\": 22.964656829833984, \"time\": 501, \"lr\": 0.0032}\n",
            "{\"epoch\": 1011, \"step\": 11121, \"loss\": 23.00664520263672, \"time\": 512, \"lr\": 0.0032}\n",
            "{\"epoch\": 1011, \"step\": 11130, \"loss\": 22.89504623413086, \"time\": 522, \"lr\": 0.0032}\n",
            "{\"epoch\": 1012, \"step\": 11132, \"loss\": 22.926177978515625, \"time\": 534, \"lr\": 0.0032}\n",
            "{\"epoch\": 1012, \"step\": 11142, \"loss\": 22.88048553466797, \"time\": 545, \"lr\": 0.0032}\n",
            "{\"epoch\": 1013, \"step\": 11143, \"loss\": 22.895219802856445, \"time\": 555, \"lr\": 0.0032}\n",
            "{\"epoch\": 1013, \"step\": 11152, \"loss\": 22.867467880249023, \"time\": 566, \"lr\": 0.0032}\n",
            "{\"epoch\": 1014, \"step\": 11154, \"loss\": 22.78415870666504, \"time\": 577, \"lr\": 0.0032}\n",
            "{\"epoch\": 1014, \"step\": 11164, \"loss\": 22.795055389404297, \"time\": 588, \"lr\": 0.0032}\n",
            "{\"epoch\": 1015, \"step\": 11165, \"loss\": 22.761682510375977, \"time\": 599, \"lr\": 0.0032}\n",
            "{\"epoch\": 1015, \"step\": 11174, \"loss\": 22.676565170288086, \"time\": 609, \"lr\": 0.0032}\n",
            "{\"epoch\": 1016, \"step\": 11176, \"loss\": 22.759885787963867, \"time\": 620, \"lr\": 0.0032}\n",
            "{\"epoch\": 1016, \"step\": 11186, \"loss\": 22.767274856567383, \"time\": 631, \"lr\": 0.0032}\n",
            "{\"epoch\": 1017, \"step\": 11187, \"loss\": 22.722869873046875, \"time\": 641, \"lr\": 0.0032}\n",
            "{\"epoch\": 1017, \"step\": 11197, \"loss\": 22.622516632080078, \"time\": 652, \"lr\": 0.0032}\n",
            "{\"epoch\": 1018, \"step\": 11198, \"loss\": 22.71770668029785, \"time\": 663, \"lr\": 0.0032}\n",
            "{\"epoch\": 1018, \"step\": 11207, \"loss\": 22.602455139160156, \"time\": 673, \"lr\": 0.0032}\n",
            "{\"epoch\": 1019, \"step\": 11209, \"loss\": 22.663532257080078, \"time\": 684, \"lr\": 0.0032}\n",
            "{\"epoch\": 1019, \"step\": 11219, \"loss\": 22.618675231933594, \"time\": 695, \"lr\": 0.0032}\n",
            "{\"epoch\": 1020, \"step\": 11220, \"loss\": 22.554439544677734, \"time\": 706, \"lr\": 0.0032}\n",
            "{\"epoch\": 1020, \"step\": 11229, \"loss\": 22.625221252441406, \"time\": 716, \"lr\": 0.0032}\n",
            "{\"epoch\": 1021, \"step\": 11231, \"loss\": 22.558969497680664, \"time\": 727, \"lr\": 0.0032}\n",
            "{\"epoch\": 1021, \"step\": 11240, \"loss\": 22.59861946105957, \"time\": 737, \"lr\": 0.0032}\n",
            "{\"epoch\": 1022, \"step\": 11242, \"loss\": 22.54389190673828, \"time\": 749, \"lr\": 0.0032}\n",
            "{\"epoch\": 1022, \"step\": 11252, \"loss\": 22.519990921020508, \"time\": 760, \"lr\": 0.0032}\n",
            "{\"epoch\": 1023, \"step\": 11253, \"loss\": 22.597822189331055, \"time\": 770, \"lr\": 0.0032}\n",
            "{\"epoch\": 1023, \"step\": 11262, \"loss\": 22.563385009765625, \"time\": 781, \"lr\": 0.0032}\n",
            "{\"epoch\": 1024, \"step\": 11264, \"loss\": 22.54405975341797, \"time\": 793, \"lr\": 0.0032}\n",
            "{\"epoch\": 1024, \"step\": 11274, \"loss\": 22.429340362548828, \"time\": 804, \"lr\": 0.0032}\n",
            "{\"epoch\": 1025, \"step\": 11275, \"loss\": 22.48927879333496, \"time\": 815, \"lr\": 0.0032}\n",
            "{\"epoch\": 1025, \"step\": 11285, \"loss\": 22.41213607788086, \"time\": 826, \"lr\": 0.0032}\n",
            "{\"epoch\": 1026, \"step\": 11286, \"loss\": 22.463951110839844, \"time\": 836, \"lr\": 0.0032}\n",
            "{\"epoch\": 1026, \"step\": 11295, \"loss\": 22.48188591003418, \"time\": 847, \"lr\": 0.0032}\n",
            "{\"epoch\": 1027, \"step\": 11297, \"loss\": 22.487857818603516, \"time\": 858, \"lr\": 0.0032}\n",
            "{\"epoch\": 1027, \"step\": 11306, \"loss\": 22.474002838134766, \"time\": 868, \"lr\": 0.0032}\n",
            "{\"epoch\": 1028, \"step\": 11308, \"loss\": 22.512065887451172, \"time\": 879, \"lr\": 0.0032}\n",
            "{\"epoch\": 1028, \"step\": 11318, \"loss\": 22.402278900146484, \"time\": 890, \"lr\": 0.0032}\n",
            "{\"epoch\": 1029, \"step\": 11319, \"loss\": 22.37925910949707, \"time\": 900, \"lr\": 0.0032}\n",
            "{\"epoch\": 1029, \"step\": 11328, \"loss\": 22.48189926147461, \"time\": 911, \"lr\": 0.0032}\n",
            "{\"epoch\": 1030, \"step\": 11330, \"loss\": 22.436786651611328, \"time\": 923, \"lr\": 0.0032}\n",
            "{\"epoch\": 1030, \"step\": 11340, \"loss\": 22.409021377563477, \"time\": 934, \"lr\": 0.0032}\n",
            "{\"epoch\": 1031, \"step\": 11341, \"loss\": 22.399595260620117, \"time\": 944, \"lr\": 0.0032}\n",
            "{\"epoch\": 1031, \"step\": 11350, \"loss\": 22.30878257751465, \"time\": 955, \"lr\": 0.0032}\n",
            "{\"epoch\": 1032, \"step\": 11352, \"loss\": 22.36652374267578, \"time\": 967, \"lr\": 0.0032}\n",
            "{\"epoch\": 1032, \"step\": 11361, \"loss\": 22.373544692993164, \"time\": 977, \"lr\": 0.0032}\n",
            "{\"epoch\": 1033, \"step\": 11363, \"loss\": 22.427045822143555, \"time\": 988, \"lr\": 0.0032}\n",
            "{\"epoch\": 1033, \"step\": 11373, \"loss\": 22.3877010345459, \"time\": 999, \"lr\": 0.0032}\n",
            "{\"epoch\": 1034, \"step\": 11374, \"loss\": 22.33385467529297, \"time\": 1010, \"lr\": 0.0032}\n",
            "{\"epoch\": 1034, \"step\": 11383, \"loss\": 22.329742431640625, \"time\": 1020, \"lr\": 0.0032}\n",
            "{\"epoch\": 1035, \"step\": 11385, \"loss\": 22.3303165435791, \"time\": 1031, \"lr\": 0.0032}\n",
            "{\"epoch\": 1035, \"step\": 11394, \"loss\": 22.36856460571289, \"time\": 1042, \"lr\": 0.0032}\n",
            "{\"epoch\": 1036, \"step\": 11396, \"loss\": 22.32288360595703, \"time\": 1053, \"lr\": 0.0032}\n",
            "{\"epoch\": 1036, \"step\": 11405, \"loss\": 22.357383728027344, \"time\": 1063, \"lr\": 0.0032}\n",
            "{\"epoch\": 1037, \"step\": 11407, \"loss\": 22.254188537597656, \"time\": 1074, \"lr\": 0.0032}\n",
            "{\"epoch\": 1037, \"step\": 11416, \"loss\": 22.274871826171875, \"time\": 1085, \"lr\": 0.0032}\n",
            "{\"epoch\": 1038, \"step\": 11418, \"loss\": 22.274673461914062, \"time\": 1096, \"lr\": 0.0032}\n",
            "{\"epoch\": 1038, \"step\": 11427, \"loss\": 22.332551956176758, \"time\": 1106, \"lr\": 0.0032}\n",
            "{\"epoch\": 1039, \"step\": 11429, \"loss\": 22.25531578063965, \"time\": 1118, \"lr\": 0.0032}\n",
            "{\"epoch\": 1039, \"step\": 11438, \"loss\": 22.310579299926758, \"time\": 1128, \"lr\": 0.0032}\n",
            "{\"epoch\": 1040, \"step\": 11440, \"loss\": 22.292400360107422, \"time\": 1139, \"lr\": 0.0032}\n",
            "{\"epoch\": 1040, \"step\": 11449, \"loss\": 22.258098602294922, \"time\": 1150, \"lr\": 0.0032}\n",
            "{\"epoch\": 1041, \"step\": 11451, \"loss\": 22.244037628173828, \"time\": 1161, \"lr\": 0.0032}\n",
            "{\"epoch\": 1041, \"step\": 11461, \"loss\": 22.157012939453125, \"time\": 1172, \"lr\": 0.0032}\n",
            "{\"epoch\": 1042, \"step\": 11462, \"loss\": 22.29653549194336, \"time\": 1182, \"lr\": 0.0032}\n",
            "{\"epoch\": 1042, \"step\": 11471, \"loss\": 22.167682647705078, \"time\": 1192, \"lr\": 0.0032}\n",
            "{\"epoch\": 1043, \"step\": 11473, \"loss\": 22.170324325561523, \"time\": 1204, \"lr\": 0.0032}\n",
            "{\"epoch\": 1043, \"step\": 11482, \"loss\": 22.196208953857422, \"time\": 1214, \"lr\": 0.0032}\n",
            "{\"epoch\": 1044, \"step\": 11484, \"loss\": 22.201818466186523, \"time\": 1226, \"lr\": 0.0032}\n",
            "{\"epoch\": 1044, \"step\": 11494, \"loss\": 22.03048324584961, \"time\": 1237, \"lr\": 0.0032}\n",
            "{\"epoch\": 1045, \"step\": 11495, \"loss\": 22.237308502197266, \"time\": 1247, \"lr\": 0.0032}\n",
            "{\"epoch\": 1045, \"step\": 11504, \"loss\": 22.228652954101562, \"time\": 1258, \"lr\": 0.0032}\n",
            "{\"epoch\": 1046, \"step\": 11506, \"loss\": 22.243783950805664, \"time\": 1269, \"lr\": 0.0032}\n",
            "{\"epoch\": 1046, \"step\": 11515, \"loss\": 22.221149444580078, \"time\": 1279, \"lr\": 0.0032}\n",
            "{\"epoch\": 1047, \"step\": 11517, \"loss\": 22.23213005065918, \"time\": 1290, \"lr\": 0.0032}\n",
            "{\"epoch\": 1047, \"step\": 11527, \"loss\": 22.12363624572754, \"time\": 1301, \"lr\": 0.0032}\n",
            "{\"epoch\": 1048, \"step\": 11528, \"loss\": 22.19099235534668, \"time\": 1311, \"lr\": 0.0032}\n",
            "{\"epoch\": 1048, \"step\": 11537, \"loss\": 22.144437789916992, \"time\": 1322, \"lr\": 0.0032}\n",
            "{\"epoch\": 1049, \"step\": 11539, \"loss\": 22.203718185424805, \"time\": 1333, \"lr\": 0.0032}\n",
            "{\"epoch\": 1049, \"step\": 11548, \"loss\": 22.213069915771484, \"time\": 1343, \"lr\": 0.0032}\n",
            "{\"epoch\": 1050, \"step\": 11550, \"loss\": 22.143878936767578, \"time\": 1355, \"lr\": 0.0032}\n",
            "{\"epoch\": 1050, \"step\": 11559, \"loss\": 22.113176345825195, \"time\": 1365, \"lr\": 0.0032}\n",
            "{\"epoch\": 1051, \"step\": 11561, \"loss\": 22.125959396362305, \"time\": 1376, \"lr\": 0.0032}\n",
            "{\"epoch\": 1051, \"step\": 11570, \"loss\": 22.185365676879883, \"time\": 1386, \"lr\": 0.0032}\n",
            "{\"epoch\": 1052, \"step\": 11572, \"loss\": 22.165401458740234, \"time\": 1398, \"lr\": 0.0032}\n",
            "{\"epoch\": 1052, \"step\": 11582, \"loss\": 22.022571563720703, \"time\": 1409, \"lr\": 0.0032}\n",
            "{\"epoch\": 1053, \"step\": 11583, \"loss\": 22.16349983215332, \"time\": 1419, \"lr\": 0.0032}\n",
            "{\"epoch\": 1053, \"step\": 11592, \"loss\": 22.136686325073242, \"time\": 1429, \"lr\": 0.0032}\n",
            "{\"epoch\": 1054, \"step\": 11594, \"loss\": 22.15786361694336, \"time\": 1441, \"lr\": 0.0032}\n",
            "{\"epoch\": 1054, \"step\": 11603, \"loss\": 22.118507385253906, \"time\": 1451, \"lr\": 0.0032}\n",
            "{\"epoch\": 1055, \"step\": 11605, \"loss\": 22.143335342407227, \"time\": 1464, \"lr\": 0.0032}\n",
            "{\"epoch\": 1055, \"step\": 11615, \"loss\": 21.94571304321289, \"time\": 1475, \"lr\": 0.0032}\n",
            "{\"epoch\": 1056, \"step\": 11616, \"loss\": 22.03755760192871, \"time\": 1486, \"lr\": 0.0032}\n",
            "{\"epoch\": 1056, \"step\": 11625, \"loss\": 22.078290939331055, \"time\": 1496, \"lr\": 0.0032}\n",
            "{\"epoch\": 1057, \"step\": 11627, \"loss\": 22.083282470703125, \"time\": 1508, \"lr\": 0.0032}\n",
            "{\"epoch\": 1057, \"step\": 11636, \"loss\": 22.0699520111084, \"time\": 1518, \"lr\": 0.0032}\n",
            "{\"epoch\": 1058, \"step\": 11638, \"loss\": 22.110076904296875, \"time\": 1531, \"lr\": 0.0032}\n",
            "{\"epoch\": 1058, \"step\": 11648, \"loss\": 21.976234436035156, \"time\": 1542, \"lr\": 0.0032}\n",
            "{\"epoch\": 1059, \"step\": 11649, \"loss\": 22.015108108520508, \"time\": 1552, \"lr\": 0.0032}\n",
            "{\"epoch\": 1059, \"step\": 11658, \"loss\": 22.051103591918945, \"time\": 1563, \"lr\": 0.0032}\n",
            "{\"epoch\": 1060, \"step\": 11660, \"loss\": 22.039142608642578, \"time\": 1575, \"lr\": 0.0032}\n",
            "{\"epoch\": 1060, \"step\": 11670, \"loss\": 21.92642593383789, \"time\": 1585, \"lr\": 0.0032}\n",
            "{\"epoch\": 1061, \"step\": 11671, \"loss\": 22.14325523376465, \"time\": 1597, \"lr\": 0.0032}\n",
            "{\"epoch\": 1061, \"step\": 11681, \"loss\": 21.941192626953125, \"time\": 1607, \"lr\": 0.0032}\n",
            "{\"epoch\": 1062, \"step\": 11682, \"loss\": 22.04265594482422, \"time\": 1619, \"lr\": 0.0032}\n",
            "{\"epoch\": 1062, \"step\": 11691, \"loss\": 21.99677848815918, \"time\": 1629, \"lr\": 0.0032}\n",
            "{\"epoch\": 1063, \"step\": 11693, \"loss\": 22.08110809326172, \"time\": 1641, \"lr\": 0.0032}\n",
            "{\"epoch\": 1063, \"step\": 11702, \"loss\": 22.134464263916016, \"time\": 1651, \"lr\": 0.0032}\n",
            "{\"epoch\": 1064, \"step\": 11704, \"loss\": 22.075231552124023, \"time\": 1663, \"lr\": 0.0032}\n",
            "{\"epoch\": 1064, \"step\": 11714, \"loss\": 21.886795043945312, \"time\": 1674, \"lr\": 0.0032}\n",
            "{\"epoch\": 1065, \"step\": 11715, \"loss\": 21.926023483276367, \"time\": 1685, \"lr\": 0.0032}\n",
            "{\"epoch\": 1065, \"step\": 11725, \"loss\": 21.925127029418945, \"time\": 1696, \"lr\": 0.0032}\n",
            "{\"epoch\": 1066, \"step\": 11726, \"loss\": 21.99569320678711, \"time\": 1707, \"lr\": 0.0032}\n",
            "{\"epoch\": 1066, \"step\": 11735, \"loss\": 22.10016632080078, \"time\": 1717, \"lr\": 0.0032}\n",
            "{\"epoch\": 1067, \"step\": 11737, \"loss\": 22.08254051208496, \"time\": 1729, \"lr\": 0.0032}\n",
            "{\"epoch\": 1067, \"step\": 11747, \"loss\": 21.903106689453125, \"time\": 1740, \"lr\": 0.0032}\n",
            "{\"epoch\": 1068, \"step\": 11748, \"loss\": 22.112550735473633, \"time\": 1752, \"lr\": 0.0032}\n",
            "{\"epoch\": 1068, \"step\": 11758, \"loss\": 21.85489845275879, \"time\": 1762, \"lr\": 0.0032}\n",
            "{\"epoch\": 1069, \"step\": 11759, \"loss\": 22.116750717163086, \"time\": 1774, \"lr\": 0.0032}\n",
            "{\"epoch\": 1069, \"step\": 11768, \"loss\": 21.932844161987305, \"time\": 1784, \"lr\": 0.0032}\n",
            "{\"epoch\": 1070, \"step\": 11770, \"loss\": 22.00860595703125, \"time\": 1795, \"lr\": 0.0032}\n",
            "{\"epoch\": 1070, \"step\": 11780, \"loss\": 21.82846450805664, \"time\": 1806, \"lr\": 0.0032}\n",
            "{\"epoch\": 1071, \"step\": 11781, \"loss\": 22.025278091430664, \"time\": 1818, \"lr\": 0.0032}\n",
            "{\"epoch\": 1071, \"step\": 11791, \"loss\": 21.848066329956055, \"time\": 1829, \"lr\": 0.0032}\n",
            "{\"epoch\": 1072, \"step\": 11792, \"loss\": 22.061473846435547, \"time\": 1841, \"lr\": 0.0032}\n",
            "{\"epoch\": 1072, \"step\": 11801, \"loss\": 22.03411865234375, \"time\": 1852, \"lr\": 0.0032}\n",
            "{\"epoch\": 1073, \"step\": 11803, \"loss\": 21.963478088378906, \"time\": 1863, \"lr\": 0.0032}\n",
            "{\"epoch\": 1073, \"step\": 11813, \"loss\": 21.81344985961914, \"time\": 1874, \"lr\": 0.0032}\n",
            "{\"epoch\": 1074, \"step\": 11814, \"loss\": 21.920169830322266, \"time\": 1886, \"lr\": 0.0032}\n",
            "{\"epoch\": 1074, \"step\": 11824, \"loss\": 21.959184646606445, \"time\": 1896, \"lr\": 0.0032}\n",
            "{\"epoch\": 1075, \"step\": 11825, \"loss\": 21.956531524658203, \"time\": 1907, \"lr\": 0.0032}\n",
            "{\"epoch\": 1075, \"step\": 11834, \"loss\": 21.961627960205078, \"time\": 1917, \"lr\": 0.0032}\n",
            "{\"epoch\": 1076, \"step\": 11836, \"loss\": 22.00013542175293, \"time\": 1930, \"lr\": 0.0032}\n",
            "{\"epoch\": 1076, \"step\": 11845, \"loss\": 21.942514419555664, \"time\": 1940, \"lr\": 0.0032}\n",
            "{\"epoch\": 1077, \"step\": 11847, \"loss\": 22.013792037963867, \"time\": 1952, \"lr\": 0.0032}\n",
            "{\"epoch\": 1077, \"step\": 11856, \"loss\": 22.037351608276367, \"time\": 1962, \"lr\": 0.0032}\n",
            "{\"epoch\": 1078, \"step\": 11858, \"loss\": 21.963090896606445, \"time\": 1974, \"lr\": 0.0032}\n",
            "{\"epoch\": 1078, \"step\": 11867, \"loss\": 21.89158821105957, \"time\": 1984, \"lr\": 0.0032}\n",
            "{\"epoch\": 1079, \"step\": 11869, \"loss\": 21.934326171875, \"time\": 1996, \"lr\": 0.0032}\n",
            "{\"epoch\": 1079, \"step\": 11879, \"loss\": 21.83248519897461, \"time\": 2007, \"lr\": 0.0032}\n",
            "{\"epoch\": 1080, \"step\": 11880, \"loss\": 21.91637420654297, \"time\": 2019, \"lr\": 0.0032}\n",
            "{\"epoch\": 1080, \"step\": 11890, \"loss\": 21.89033317565918, \"time\": 2030, \"lr\": 0.0032}\n",
            "{\"epoch\": 1081, \"step\": 11891, \"loss\": 21.987632751464844, \"time\": 2041, \"lr\": 0.0032}\n",
            "{\"epoch\": 1081, \"step\": 11900, \"loss\": 21.908920288085938, \"time\": 2051, \"lr\": 0.0032}\n",
            "{\"epoch\": 1082, \"step\": 11902, \"loss\": 21.930301666259766, \"time\": 2064, \"lr\": 0.0032}\n",
            "{\"epoch\": 1082, \"step\": 11911, \"loss\": 22.00620460510254, \"time\": 2074, \"lr\": 0.0032}\n",
            "{\"epoch\": 1083, \"step\": 11913, \"loss\": 21.896013259887695, \"time\": 2086, \"lr\": 0.0032}\n",
            "{\"epoch\": 1083, \"step\": 11923, \"loss\": 21.78145980834961, \"time\": 2097, \"lr\": 0.0032}\n",
            "{\"epoch\": 1084, \"step\": 11924, \"loss\": 22.019311904907227, \"time\": 2108, \"lr\": 0.0032}\n",
            "{\"epoch\": 1084, \"step\": 11933, \"loss\": 21.974117279052734, \"time\": 2119, \"lr\": 0.0032}\n",
            "{\"epoch\": 1085, \"step\": 11935, \"loss\": 21.968473434448242, \"time\": 2131, \"lr\": 0.0032}\n",
            "{\"epoch\": 1085, \"step\": 11944, \"loss\": 22.013385772705078, \"time\": 2141, \"lr\": 0.0032}\n",
            "{\"epoch\": 1086, \"step\": 11946, \"loss\": 21.96038055419922, \"time\": 2153, \"lr\": 0.0032}\n",
            "{\"epoch\": 1086, \"step\": 11955, \"loss\": 21.918596267700195, \"time\": 2164, \"lr\": 0.0032}\n",
            "{\"epoch\": 1087, \"step\": 11957, \"loss\": 21.861549377441406, \"time\": 2176, \"lr\": 0.0032}\n",
            "{\"epoch\": 1087, \"step\": 11967, \"loss\": 21.749732971191406, \"time\": 2187, \"lr\": 0.0032}\n",
            "{\"epoch\": 1088, \"step\": 11968, \"loss\": 21.915573120117188, \"time\": 2198, \"lr\": 0.0032}\n",
            "{\"epoch\": 1088, \"step\": 11977, \"loss\": 21.923954010009766, \"time\": 2208, \"lr\": 0.0032}\n",
            "{\"epoch\": 1089, \"step\": 11979, \"loss\": 21.94563102722168, \"time\": 2220, \"lr\": 0.0032}\n",
            "{\"epoch\": 1089, \"step\": 11989, \"loss\": 21.68011474609375, \"time\": 2231, \"lr\": 0.0032}\n",
            "{\"epoch\": 1090, \"step\": 11990, \"loss\": 21.899911880493164, \"time\": 2243, \"lr\": 0.0032}\n",
            "{\"epoch\": 1090, \"step\": 12000, \"loss\": 21.663639068603516, \"time\": 2254, \"lr\": 0.0032}\n",
            "{\"epoch\": 1091, \"step\": 12001, \"loss\": 21.91397476196289, \"time\": 2265, \"lr\": 0.0032}\n",
            "{\"epoch\": 1091, \"step\": 12010, \"loss\": 21.872215270996094, \"time\": 2276, \"lr\": 0.0032}\n",
            "{\"epoch\": 1092, \"step\": 12012, \"loss\": 21.850786209106445, \"time\": 2288, \"lr\": 0.0032}\n",
            "{\"epoch\": 1092, \"step\": 12021, \"loss\": 21.890625, \"time\": 2298, \"lr\": 0.0032}\n",
            "{\"epoch\": 1093, \"step\": 12023, \"loss\": 21.87718963623047, \"time\": 2311, \"lr\": 0.0032}\n",
            "{\"epoch\": 1093, \"step\": 12033, \"loss\": 21.733945846557617, \"time\": 2321, \"lr\": 0.0032}\n",
            "{\"epoch\": 1094, \"step\": 12034, \"loss\": 21.906322479248047, \"time\": 2333, \"lr\": 0.0032}\n",
            "{\"epoch\": 1094, \"step\": 12043, \"loss\": 21.867984771728516, \"time\": 2343, \"lr\": 0.0032}\n",
            "{\"epoch\": 1095, \"step\": 12045, \"loss\": 21.886268615722656, \"time\": 2356, \"lr\": 0.0032}\n",
            "{\"epoch\": 1095, \"step\": 12054, \"loss\": 21.875404357910156, \"time\": 2366, \"lr\": 0.0032}\n",
            "{\"epoch\": 1096, \"step\": 12056, \"loss\": 21.937999725341797, \"time\": 2379, \"lr\": 0.0032}\n",
            "{\"epoch\": 1096, \"step\": 12066, \"loss\": 21.6695499420166, \"time\": 2389, \"lr\": 0.0032}\n",
            "{\"epoch\": 1097, \"step\": 12067, \"loss\": 21.90334701538086, \"time\": 2401, \"lr\": 0.0032}\n",
            "{\"epoch\": 1097, \"step\": 12077, \"loss\": 21.649093627929688, \"time\": 2412, \"lr\": 0.0032}\n",
            "{\"epoch\": 1098, \"step\": 12078, \"loss\": 21.854564666748047, \"time\": 2423, \"lr\": 0.0032}\n",
            "{\"epoch\": 1098, \"step\": 12087, \"loss\": 21.82748031616211, \"time\": 2433, \"lr\": 0.0032}\n",
            "{\"epoch\": 1099, \"step\": 12089, \"loss\": 21.814754486083984, \"time\": 2445, \"lr\": 0.0032}\n",
            "{\"epoch\": 1099, \"step\": 12099, \"loss\": 21.751996994018555, \"time\": 2456, \"lr\": 0.0032}\n"
          ]
        }
      ]
    }
  ]
}