{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBEbCQozQ9UP7vYoRtjYm9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larissabooth/cv_project/blob/main/evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqVM6wqc3k3-",
        "outputId": "771ef554-a85c-4270-85b8-cd19bdf57d5a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "\n",
        "# All rights reserved.\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import signal\n",
        "import sys\n",
        "import time\n",
        "import urllib\n",
        "\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "\n",
        "sys.path.append(\"/content/drive/My Drive/computer_vision_project/vicreg\")\n",
        "import resnet\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "LSwD6err3tMt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Network configurations\n",
        "\n",
        "#Data\n",
        "data_dir = \"/content/drive/My Drive/computer_vision_project/Kitchener_torch\"\n",
        "train_percent = 10 #\"size of traing set in percent\"\n",
        "\n",
        "# Checkpoint\n",
        "pretrained = None #path to pretrained model\n",
        "exp_dir = \"./checkpoint/lincls/\" #path to export directory\n",
        "print_freq = 100 #number of steps before printing\n",
        "\n",
        "# Model\n",
        "arch = \"resnet50\"\n",
        "\n",
        "# Optim\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "lr_backbone = 0.0 #\"backbone base learning rate\"\n",
        "lr_head = 0.3 #\"classifier base learning rate\"\n",
        "weight_decay = 1e-6\n",
        "weights = \"freeze\" #(\"finetune\", \"freeze\")\n",
        "\n",
        "# Running\n",
        "workers= 1 #\"number of data loader workers\""
      ],
      "metadata": {
        "id": "IqF1zEYB4rGY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-Pa58iE3c9I"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    parser = get_arguments()\n",
        "    args = parser.parse_args()\n",
        "    if args.train_percent in {1, 10}:\n",
        "        args.train_files = urllib.request.urlopen(\n",
        "            f\"https://raw.githubusercontent.com/google-research/simclr/master/imagenet_subsets/{args.train_percent}percent.txt\"\n",
        "        ).readlines()\n",
        "    args.ngpus_per_node = torch.cuda.device_count()\n",
        "    if \"SLURM_JOB_ID\" in os.environ:\n",
        "        signal.signal(signal.SIGUSR1, handle_sigusr1)\n",
        "        signal.signal(signal.SIGTERM, handle_sigterm)\n",
        "    # single-node distributed training\n",
        "    args.rank = 0\n",
        "    args.dist_url = f\"tcp://localhost:{random.randrange(49152, 65535)}\"\n",
        "    args.world_size = args.ngpus_per_node\n",
        "    torch.multiprocessing.spawn(main_worker, (args,), args.ngpus_per_node)\n",
        "\n",
        "\n",
        "def main_worker(gpu, args):\n",
        "    args.rank += gpu\n",
        "    torch.distributed.init_process_group(\n",
        "        backend=\"nccl\",\n",
        "        init_method=args.dist_url,\n",
        "        world_size=args.world_size,\n",
        "        rank=args.rank,\n",
        "    )\n",
        "\n",
        "    if args.rank == 0:\n",
        "        args.exp_dir.mkdir(parents=True, exist_ok=True)\n",
        "        stats_file = open(args.exp_dir / \"stats.txt\", \"a\", buffering=1)\n",
        "        print(\" \".join(sys.argv))\n",
        "        print(\" \".join(sys.argv), file=stats_file)\n",
        "\n",
        "    torch.cuda.set_device(gpu)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    backbone, embedding = resnet.__dict__[args.arch](zero_init_residual=True)\n",
        "    state_dict = torch.load(args.pretrained, map_location=\"cpu\")\n",
        "    if \"model\" in state_dict:\n",
        "        state_dict = state_dict[\"model\"]\n",
        "        state_dict = {\n",
        "            key.replace(\"module.backbone.\", \"\"): value\n",
        "            for (key, value) in state_dict.items()\n",
        "        }\n",
        "    backbone.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    head = nn.Linear(embedding, 1000)\n",
        "    head.weight.data.normal_(mean=0.0, std=0.01)\n",
        "    head.bias.data.zero_()\n",
        "    model = nn.Sequential(backbone, head)\n",
        "    model.cuda(gpu)\n",
        "\n",
        "    if args.weights == \"freeze\":\n",
        "        backbone.requires_grad_(False)\n",
        "        head.requires_grad_(True)\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
        "\n",
        "    param_groups = [dict(params=head.parameters(), lr=args.lr_head)]\n",
        "    if args.weights == \"finetune\":\n",
        "        param_groups.append(dict(params=backbone.parameters(), lr=args.lr_backbone))\n",
        "    optimizer = optim.SGD(param_groups, 0, momentum=0.9, weight_decay=args.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)\n",
        "\n",
        "    # automatically resume from checkpoint if it exists\n",
        "    if (args.exp_dir / \"checkpoint.pth\").is_file():\n",
        "        ckpt = torch.load(args.exp_dir / \"checkpoint.pth\", map_location=\"cpu\")\n",
        "        start_epoch = ckpt[\"epoch\"]\n",
        "        best_acc = ckpt[\"best_acc\"]\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "        scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "        best_acc = argparse.Namespace(top1=0, top5=0)\n",
        "\n",
        "    # Data loading code\n",
        "    traindir = args.data_dir / \"train\"\n",
        "    valdir = args.data_dir / \"val\"\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(\n",
        "        traindir,\n",
        "        transforms.Compose(\n",
        "            [\n",
        "                transforms.RandomResizedCrop(224),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ]\n",
        "        ),\n",
        "    )\n",
        "    val_dataset = datasets.ImageFolder(\n",
        "        valdir,\n",
        "        transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    if args.train_percent in {1, 10}:\n",
        "        train_dataset.samples = []\n",
        "        for fname in args.train_files:\n",
        "            fname = fname.decode().strip()\n",
        "            cls = fname.split(\"_\")[0]\n",
        "            train_dataset.samples.append(\n",
        "                (traindir / cls / fname, train_dataset.class_to_idx[cls])\n",
        "            )\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
        "    kwargs = dict(\n",
        "        batch_size=args.batch_size // args.world_size,\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, sampler=train_sampler, **kwargs\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, **kwargs)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "        # train\n",
        "        if args.weights == \"finetune\":\n",
        "            model.train()\n",
        "        elif args.weights == \"freeze\":\n",
        "            model.eval()\n",
        "        else:\n",
        "            assert False\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        for step, (images, target) in enumerate(\n",
        "            train_loader, start=epoch * len(train_loader)\n",
        "        ):\n",
        "            output = model(images.cuda(gpu, non_blocking=True))\n",
        "            loss = criterion(output, target.cuda(gpu, non_blocking=True))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if step % args.print_freq == 0:\n",
        "                torch.distributed.reduce(loss.div_(args.world_size), 0)\n",
        "                if args.rank == 0:\n",
        "                    pg = optimizer.param_groups\n",
        "                    lr_head = pg[0][\"lr\"]\n",
        "                    lr_backbone = pg[1][\"lr\"] if len(pg) == 2 else 0\n",
        "                    stats = dict(\n",
        "                        epoch=epoch,\n",
        "                        step=step,\n",
        "                        lr_backbone=lr_backbone,\n",
        "                        lr_head=lr_head,\n",
        "                        loss=loss.item(),\n",
        "                        time=int(time.time() - start_time),\n",
        "                    )\n",
        "                    print(json.dumps(stats))\n",
        "                    print(json.dumps(stats), file=stats_file)\n",
        "\n",
        "        # evaluate\n",
        "        model.eval()\n",
        "        if args.rank == 0:\n",
        "            top1 = AverageMeter(\"Acc@1\")\n",
        "            top5 = AverageMeter(\"Acc@5\")\n",
        "            with torch.no_grad():\n",
        "                for images, target in val_loader:\n",
        "                    output = model(images.cuda(gpu, non_blocking=True))\n",
        "                    acc1, acc5 = accuracy(\n",
        "                        output, target.cuda(gpu, non_blocking=True), topk=(1, 5)\n",
        "                    )\n",
        "                    top1.update(acc1[0].item(), images.size(0))\n",
        "                    top5.update(acc5[0].item(), images.size(0))\n",
        "            best_acc.top1 = max(best_acc.top1, top1.avg)\n",
        "            best_acc.top5 = max(best_acc.top5, top5.avg)\n",
        "            stats = dict(\n",
        "                epoch=epoch,\n",
        "                acc1=top1.avg,\n",
        "                acc5=top5.avg,\n",
        "                best_acc1=best_acc.top1,\n",
        "                best_acc5=best_acc.top5,\n",
        "            )\n",
        "            print(json.dumps(stats))\n",
        "            print(json.dumps(stats), file=stats_file)\n",
        "\n",
        "        scheduler.step()\n",
        "        if args.rank == 0:\n",
        "            state = dict(\n",
        "                epoch=epoch + 1,\n",
        "                best_acc=best_acc,\n",
        "                model=model.state_dict(),\n",
        "                optimizer=optimizer.state_dict(),\n",
        "                scheduler=scheduler.state_dict(),\n",
        "            )\n",
        "            torch.save(state, args.exp_dir / \"checkpoint.pth\")\n",
        "\n",
        "\n",
        "def handle_sigusr1(signum, frame):\n",
        "    os.system(f'scontrol requeue {os.getenv(\"SLURM_JOB_ID\")}')\n",
        "    exit()\n",
        "\n",
        "\n",
        "def handle_sigterm(signum, frame):\n",
        "    pass\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self, name, fmt=\":f\"):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "\n",
        "# All rights reserved.\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "\n",
        "[ ]\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}